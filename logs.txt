* 
* ==> Audit <==
* |-----------|------------------------------|----------|-----------|---------|---------------------|---------------------|
|  Command  |             Args             | Profile  |   User    | Version |     Start Time      |      End Time       |
|-----------|------------------------------|----------|-----------|---------|---------------------|---------------------|
| dashboard |                              | minikube | budescode | v1.30.1 | 24 Nov 23 11:30 WAT |                     |
| dashboard |                              | minikube | budescode | v1.30.1 | 24 Nov 23 11:48 WAT |                     |
| service   | django-service               | minikube | budescode | v1.30.1 | 24 Nov 23 12:14 WAT | 24 Nov 23 12:14 WAT |
| dashboard |                              | minikube | budescode | v1.30.1 | 24 Nov 23 12:16 WAT |                     |
| image     | load django-app-v1:1.0.1     | minikube | budescode | v1.30.1 | 24 Nov 23 12:29 WAT | 24 Nov 23 12:30 WAT |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 12:31 WAT | 24 Nov 23 12:31 WAT |
| service   | django-service               | minikube | budescode | v1.30.1 | 24 Nov 23 12:32 WAT | 24 Nov 23 12:32 WAT |
| service   | django-service               | minikube | budescode | v1.30.1 | 24 Nov 23 12:34 WAT | 24 Nov 23 12:34 WAT |
| ssh       |                              | minikube | budescode | v1.30.1 | 24 Nov 23 13:01 WAT |                     |
| image     | load                         | minikube | budescode | v1.30.1 | 24 Nov 23 13:02 WAT |                     |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:02 WAT | 24 Nov 23 13:02 WAT |
| image     | load                         | minikube | budescode | v1.30.1 | 24 Nov 23 13:03 WAT |                     |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:03 WAT | 24 Nov 23 13:03 WAT |
| ssh       |                              | minikube | budescode | v1.30.1 | 24 Nov 23 13:03 WAT |                     |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:08 WAT | 24 Nov 23 13:08 WAT |
| image     | load django-app-v1:1.0.0     | minikube | budescode | v1.30.1 | 24 Nov 23 13:17 WAT | 24 Nov 23 13:19 WAT |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:19 WAT | 24 Nov 23 13:19 WAT |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:20 WAT | 24 Nov 23 13:20 WAT |
| image     | load django-app-v1:1.0.0     | minikube | budescode | v1.30.1 | 24 Nov 23 13:20 WAT | 24 Nov 23 13:21 WAT |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:20 WAT | 24 Nov 23 13:20 WAT |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:21 WAT | 24 Nov 23 13:21 WAT |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:32 WAT | 24 Nov 23 13:32 WAT |
| dashboard |                              | minikube | budescode | v1.30.1 | 24 Nov 23 13:32 WAT |                     |
| start     |                              | minikube | budescode | v1.30.1 | 24 Nov 23 13:32 WAT | 24 Nov 23 13:34 WAT |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:35 WAT | 24 Nov 23 13:35 WAT |
| image     | load django-app-v1:1.0.0     | minikube | budescode | v1.30.1 | 24 Nov 23 13:35 WAT | 24 Nov 23 13:37 WAT |
| image     | ls                           | minikube | budescode | v1.30.1 | 24 Nov 23 13:39 WAT | 24 Nov 23 13:39 WAT |
| service   | django-service               | minikube | budescode | v1.30.1 | 24 Nov 23 13:47 WAT | 24 Nov 23 13:47 WAT |
| dashboard |                              | minikube | budescode | v1.30.1 | 24 Nov 23 14:43 WAT |                     |
| image     | load django-app-v1:1.0.1     | minikube | budescode | v1.30.1 | 24 Nov 23 14:57 WAT | 24 Nov 23 14:58 WAT |
| service   | django-service               | minikube | budescode | v1.30.1 | 24 Nov 23 15:02 WAT | 24 Nov 23 15:02 WAT |
| ip        |                              | minikube | budescode | v1.30.1 | 25 Nov 23 21:09 WAT |                     |
| start     |                              | minikube | budescode | v1.30.1 | 25 Nov 23 21:09 WAT | 25 Nov 23 21:10 WAT |
| ip        |                              | minikube | budescode | v1.30.1 | 25 Nov 23 21:10 WAT | 25 Nov 23 21:10 WAT |
| image     | load django-app-v1:1.0.5     | minikube | budescode | v1.30.1 | 25 Nov 23 21:12 WAT | 25 Nov 23 21:13 WAT |
| dashboard |                              | minikube | budescode | v1.30.1 | 25 Nov 23 21:18 WAT |                     |
| ip        |                              | minikube | budescode | v1.30.1 | 25 Nov 23 21:25 WAT | 25 Nov 23 21:25 WAT |
| service   |                              | minikube | budescode | v1.30.1 | 25 Nov 23 21:25 WAT |                     |
| service   | django-service               | minikube | budescode | v1.30.1 | 25 Nov 23 21:25 WAT | 25 Nov 23 21:25 WAT |
| ip        |                              | minikube | budescode | v1.30.1 | 25 Nov 23 21:30 WAT | 25 Nov 23 21:30 WAT |
| start     |                              | minikube | budescode | v1.30.1 | 07 Dec 23 10:30 WAT | 07 Dec 23 10:31 WAT |
| dashboard |                              | minikube | budescode | v1.30.1 | 07 Dec 23 10:32 WAT |                     |
| service   | django-service               | minikube | budescode | v1.30.1 | 07 Dec 23 11:12 WAT | 07 Dec 23 11:12 WAT |
| service   | django-service               | minikube | budescode | v1.30.1 | 07 Dec 23 11:12 WAT | 07 Dec 23 11:12 WAT |
| service   | django-service               | minikube | budescode | v1.30.1 | 07 Dec 23 11:49 WAT | 07 Dec 23 11:49 WAT |
| dashboard |                              | minikube | budescode | v1.30.1 | 07 Dec 23 14:15 WAT |                     |
| delete    | deployment django-deployment | minikube | budescode | v1.30.1 | 07 Dec 23 14:16 WAT |                     |
| service   | django-service               | minikube | budescode | v1.30.1 | 07 Dec 23 14:19 WAT | 07 Dec 23 14:19 WAT |
| start     |                              | minikube | budescode | v1.30.1 | 11 Dec 23 08:40 WAT | 11 Dec 23 08:41 WAT |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 11 Dec 23 08:42 WAT |                     |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 11 Dec 23 09:16 WAT |                     |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 11 Dec 23 09:23 WAT |                     |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 11 Dec 23 09:34 WAT |                     |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 11 Dec 23 10:23 WAT |                     |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 11 Dec 23 10:30 WAT |                     |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 11 Dec 23 10:54 WAT |                     |
| start     |                              | minikube | budescode | v1.30.1 | 11 Dec 23 10:55 WAT | 11 Dec 23 10:55 WAT |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 11 Dec 23 10:56 WAT |                     |
| start     |                              | minikube | budescode | v1.30.1 | 12 Dec 23 13:11 WAT | 12 Dec 23 13:11 WAT |
| addons    | enable ingress               | minikube | budescode | v1.30.1 | 12 Dec 23 13:22 WAT |                     |
|-----------|------------------------------|----------|-----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/12/12 13:11:21
Running on machine: budescode-HP-EliteBook-x360-830-G6
Binary: Built with gc go1.20.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1212 13:11:21.056777   42383 out.go:296] Setting OutFile to fd 1 ...
I1212 13:11:21.058030   42383 out.go:348] isatty.IsTerminal(1) = true
I1212 13:11:21.058042   42383 out.go:309] Setting ErrFile to fd 2...
I1212 13:11:21.058054   42383 out.go:348] isatty.IsTerminal(2) = true
I1212 13:11:21.058261   42383 root.go:336] Updating PATH: /home/budescode/.minikube/bin
W1212 13:11:21.058681   42383 root.go:312] Error reading config file at /home/budescode/.minikube/config/config.json: open /home/budescode/.minikube/config/config.json: no such file or directory
I1212 13:11:21.059376   42383 out.go:303] Setting JSON to false
I1212 13:11:21.062945   42383 start.go:125] hostinfo: {"hostname":"budescode-HP-EliteBook-x360-830-G6","uptime":7185,"bootTime":1702375896,"procs":340,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.2.0-37-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"6c49856d-d370-40c0-b0fd-cf5ae97699b0"}
I1212 13:11:21.063033   42383 start.go:135] virtualization: kvm host
I1212 13:11:21.065902   42383 out.go:177] üòÑ  minikube v1.30.1 on Ubuntu 22.04
I1212 13:11:21.068523   42383 notify.go:220] Checking for updates...
I1212 13:11:21.069141   42383 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I1212 13:11:21.070839   42383 driver.go:375] Setting default libvirt URI to qemu:///system
I1212 13:11:21.153827   42383 docker.go:121] docker version: linux-24.0.4:Docker Engine - Community
I1212 13:11:21.153921   42383 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1212 13:11:21.525691   42383 info.go:266] docker info: {ID:e9173266-971d-4dc5-9b99-4ba0f6efd5b9 Containers:7 ContainersRunning:0 ContainersPaused:0 ContainersStopped:7 Images:14 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:37 SystemTime:2023-12-12 13:11:21.500555126 +0100 WAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.2.0-37-generic OperatingSystem:Ubuntu 22.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16568668160 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:budescode-HP-EliteBook-x360-830-G6 Labels:[] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.19.1]] Warnings:<nil>}}
I1212 13:11:21.525994   42383 docker.go:294] overlay module found
I1212 13:11:21.530579   42383 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1212 13:11:21.532707   42383 start.go:295] selected driver: docker
I1212 13:11:21.532724   42383 start.go:870] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/budescode:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I1212 13:11:21.532952   42383 start.go:881] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1212 13:11:21.533231   42383 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1212 13:11:21.613452   42383 info.go:266] docker info: {ID:e9173266-971d-4dc5-9b99-4ba0f6efd5b9 Containers:7 ContainersRunning:0 ContainersPaused:0 ContainersStopped:7 Images:14 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:37 SystemTime:2023-12-12 13:11:21.605816498 +0100 WAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.2.0-37-generic OperatingSystem:Ubuntu 22.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16568668160 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:budescode-HP-EliteBook-x360-830-G6 Labels:[] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.19.1]] Warnings:<nil>}}
I1212 13:11:21.614627   42383 cni.go:84] Creating CNI manager for ""
I1212 13:11:21.614640   42383 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1212 13:11:21.614649   42383 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/budescode:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I1212 13:11:21.616729   42383 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1212 13:11:21.619490   42383 cache.go:120] Beginning downloading kic base image for docker with docker
I1212 13:11:21.621470   42383 out.go:177] üöú  Pulling base image ...
I1212 13:11:21.623796   42383 preload.go:132] Checking if preload exists for k8s version v1.26.3 and runtime docker
I1212 13:11:21.623850   42383 preload.go:148] Found local preload: /home/budescode/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.3-docker-overlay2-amd64.tar.lz4
I1212 13:11:21.623857   42383 cache.go:57] Caching tarball of preloaded images
I1212 13:11:21.623857   42383 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 in local docker daemon
I1212 13:11:21.623978   42383 preload.go:174] Found /home/budescode/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1212 13:11:21.623984   42383 cache.go:60] Finished verifying existence of preloaded tar for  v1.26.3 on docker
I1212 13:11:21.624067   42383 profile.go:148] Saving config to /home/budescode/.minikube/profiles/minikube/config.json ...
I1212 13:11:21.643460   42383 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 in local docker daemon, skipping pull
I1212 13:11:21.643473   42383 cache.go:143] gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 exists in daemon, skipping load
I1212 13:11:21.643484   42383 cache.go:193] Successfully downloaded all kic artifacts
I1212 13:11:21.643507   42383 start.go:364] acquiring machines lock for minikube: {Name:mkfc5f68f0a5a9ae9a2d7411d58b1e019d772f96 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1212 13:11:21.643645   42383 start.go:368] acquired machines lock for "minikube" in 124.721¬µs
I1212 13:11:21.643659   42383 start.go:96] Skipping create...Using existing machine configuration
I1212 13:11:21.643662   42383 fix.go:55] fixHost starting: 
I1212 13:11:21.643891   42383 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:11:21.672642   42383 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1212 13:11:21.672659   42383 fix.go:129] unexpected machine state, will restart: <nil>
I1212 13:11:21.676012   42383 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1212 13:11:21.677823   42383 cli_runner.go:164] Run: docker start minikube
I1212 13:11:22.035791   42383 lock.go:35] WriteFile acquiring /home/budescode/.minikube/last_update_check: {Name:mk226379fbfe9d9076ea2e1bdf222036565c632c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 13:11:22.039294   42383 out.go:177] üéâ  minikube 1.32.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.32.0
I1212 13:11:22.041490   42383 out.go:177] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I1212 13:11:22.153816   42383 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:11:22.170690   42383 kic.go:426] container "minikube" state is running.
I1212 13:11:22.171097   42383 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 13:11:22.188307   42383 profile.go:148] Saving config to /home/budescode/.minikube/profiles/minikube/config.json ...
I1212 13:11:22.188531   42383 machine.go:88] provisioning docker machine ...
I1212 13:11:22.188544   42383 ubuntu.go:169] provisioning hostname "minikube"
I1212 13:11:22.188598   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:22.206851   42383 main.go:141] libmachine: Using SSH client type: native
I1212 13:11:22.207626   42383 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1212 13:11:22.207635   42383 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1212 13:11:22.208125   42383 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:34416->127.0.0.1:32772: read: connection reset by peer
I1212 13:11:25.380520   42383 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1212 13:11:25.380586   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:25.396302   42383 main.go:141] libmachine: Using SSH client type: native
I1212 13:11:25.396685   42383 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1212 13:11:25.396696   42383 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1212 13:11:25.520531   42383 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1212 13:11:25.520546   42383 ubuntu.go:175] set auth options {CertDir:/home/budescode/.minikube CaCertPath:/home/budescode/.minikube/certs/ca.pem CaPrivateKeyPath:/home/budescode/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/budescode/.minikube/machines/server.pem ServerKeyPath:/home/budescode/.minikube/machines/server-key.pem ClientKeyPath:/home/budescode/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/budescode/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/budescode/.minikube}
I1212 13:11:25.520562   42383 ubuntu.go:177] setting up certificates
I1212 13:11:25.520573   42383 provision.go:83] configureAuth start
I1212 13:11:25.520609   42383 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 13:11:25.535828   42383 provision.go:138] copyHostCerts
I1212 13:11:25.536150   42383 exec_runner.go:144] found /home/budescode/.minikube/ca.pem, removing ...
I1212 13:11:25.536158   42383 exec_runner.go:207] rm: /home/budescode/.minikube/ca.pem
I1212 13:11:25.536196   42383 exec_runner.go:151] cp: /home/budescode/.minikube/certs/ca.pem --> /home/budescode/.minikube/ca.pem (1086 bytes)
I1212 13:11:25.536522   42383 exec_runner.go:144] found /home/budescode/.minikube/cert.pem, removing ...
I1212 13:11:25.536527   42383 exec_runner.go:207] rm: /home/budescode/.minikube/cert.pem
I1212 13:11:25.536547   42383 exec_runner.go:151] cp: /home/budescode/.minikube/certs/cert.pem --> /home/budescode/.minikube/cert.pem (1127 bytes)
I1212 13:11:25.536962   42383 exec_runner.go:144] found /home/budescode/.minikube/key.pem, removing ...
I1212 13:11:25.536967   42383 exec_runner.go:207] rm: /home/budescode/.minikube/key.pem
I1212 13:11:25.536988   42383 exec_runner.go:151] cp: /home/budescode/.minikube/certs/key.pem --> /home/budescode/.minikube/key.pem (1679 bytes)
I1212 13:11:25.537215   42383 provision.go:112] generating server cert: /home/budescode/.minikube/machines/server.pem ca-key=/home/budescode/.minikube/certs/ca.pem private-key=/home/budescode/.minikube/certs/ca-key.pem org=budescode.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1212 13:11:25.675471   42383 provision.go:172] copyRemoteCerts
I1212 13:11:25.675535   42383 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1212 13:11:25.675568   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:25.689438   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:25.798612   42383 ssh_runner.go:362] scp /home/budescode/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I1212 13:11:25.874687   42383 ssh_runner.go:362] scp /home/budescode/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I1212 13:11:25.940673   42383 ssh_runner.go:362] scp /home/budescode/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1212 13:11:25.981741   42383 provision.go:86] duration metric: configureAuth took 461.155306ms
I1212 13:11:25.981759   42383 ubuntu.go:193] setting minikube options for container-runtime
I1212 13:11:25.981969   42383 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I1212 13:11:25.982070   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:25.997992   42383 main.go:141] libmachine: Using SSH client type: native
I1212 13:11:25.998481   42383 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1212 13:11:25.998495   42383 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1212 13:11:26.140274   42383 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1212 13:11:26.140288   42383 ubuntu.go:71] root file system type: overlay
I1212 13:11:26.140479   42383 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1212 13:11:26.140562   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:26.166571   42383 main.go:141] libmachine: Using SSH client type: native
I1212 13:11:26.167338   42383 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1212 13:11:26.167456   42383 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1212 13:11:26.390552   42383 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1212 13:11:26.390794   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:26.439953   42383 main.go:141] libmachine: Using SSH client type: native
I1212 13:11:26.441455   42383 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1212 13:11:26.441504   42383 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1212 13:11:26.651843   42383 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1212 13:11:26.651874   42383 machine.go:91] provisioned docker machine in 4.463327898s
I1212 13:11:26.651921   42383 start.go:300] post-start starting for "minikube" (driver="docker")
I1212 13:11:26.651939   42383 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1212 13:11:26.652101   42383 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1212 13:11:26.652250   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:26.694449   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:26.818299   42383 ssh_runner.go:195] Run: cat /etc/os-release
I1212 13:11:26.826778   42383 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1212 13:11:26.826815   42383 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1212 13:11:26.826840   42383 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1212 13:11:26.826852   42383 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1212 13:11:26.826877   42383 filesync.go:126] Scanning /home/budescode/.minikube/addons for local assets ...
I1212 13:11:26.827368   42383 filesync.go:126] Scanning /home/budescode/.minikube/files for local assets ...
I1212 13:11:26.827704   42383 start.go:303] post-start completed in 175.765708ms
I1212 13:11:26.827841   42383 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1212 13:11:26.827972   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:26.866899   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:26.979658   42383 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1212 13:11:26.991241   42383 fix.go:57] fixHost completed within 5.347560888s
I1212 13:11:26.991266   42383 start.go:83] releasing machines lock for "minikube", held for 5.347610287s
I1212 13:11:26.991421   42383 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 13:11:27.028327   42383 ssh_runner.go:195] Run: cat /version.json
I1212 13:11:27.028381   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:27.028427   42383 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1212 13:11:27.028513   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:27.050271   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:27.050731   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:27.151908   42383 ssh_runner.go:195] Run: systemctl --version
I1212 13:11:29.452839   42383 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.424365146s)
W1212 13:11:29.452914   42383 start.go:830] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
I1212 13:11:29.453035   42383 ssh_runner.go:235] Completed: systemctl --version: (2.301087472s)
I1212 13:11:29.453350   42383 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W1212 13:11:29.453618   42383 out.go:239] ‚ùó  This container is having trouble accessing https://registry.k8s.io
W1212 13:11:29.453690   42383 out.go:239] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1212 13:11:29.469131   42383 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1212 13:11:29.530419   42383 cni.go:229] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1212 13:11:29.530519   42383 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1212 13:11:29.543381   42383 cni.go:258] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1212 13:11:29.543395   42383 start.go:481] detecting cgroup driver to use...
I1212 13:11:29.543418   42383 detect.go:199] detected "systemd" cgroup driver on host os
I1212 13:11:29.543504   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1212 13:11:29.559244   42383 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1212 13:11:29.570354   42383 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1212 13:11:29.581070   42383 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I1212 13:11:29.581124   42383 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1212 13:11:29.590973   42383 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1212 13:11:29.601067   42383 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1212 13:11:29.610416   42383 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1212 13:11:29.620195   42383 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1212 13:11:29.629258   42383 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1212 13:11:29.639104   42383 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1212 13:11:29.648733   42383 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1212 13:11:29.657850   42383 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1212 13:11:29.820904   42383 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1212 13:11:30.027516   42383 start.go:481] detecting cgroup driver to use...
I1212 13:11:30.027558   42383 detect.go:199] detected "systemd" cgroup driver on host os
I1212 13:11:30.027619   42383 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1212 13:11:30.097957   42383 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1212 13:11:30.098136   42383 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1212 13:11:30.129374   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1212 13:11:30.144231   42383 ssh_runner.go:195] Run: which cri-dockerd
I1212 13:11:30.147785   42383 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1212 13:11:30.155749   42383 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1212 13:11:30.168710   42383 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1212 13:11:30.293590   42383 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1212 13:11:30.437555   42383 docker.go:538] configuring docker to use "systemd" as cgroup driver...
I1212 13:11:30.437573   42383 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (143 bytes)
I1212 13:11:30.456455   42383 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1212 13:11:30.601057   42383 ssh_runner.go:195] Run: sudo systemctl restart docker
I1212 13:11:32.187578   42383 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.586469984s)
I1212 13:11:32.187710   42383 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1212 13:11:32.324481   42383 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1212 13:11:32.469511   42383 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1212 13:11:32.593330   42383 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1212 13:11:32.698942   42383 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1212 13:11:32.747032   42383 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1212 13:11:32.922960   42383 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1212 13:11:33.320409   42383 start.go:528] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1212 13:11:33.320775   42383 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1212 13:11:33.325922   42383 start.go:549] Will wait 60s for crictl version
I1212 13:11:33.326000   42383 ssh_runner.go:195] Run: which crictl
I1212 13:11:33.329943   42383 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1212 13:11:33.554438   42383 start.go:565] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  23.0.2
RuntimeApiVersion:  v1alpha2
I1212 13:11:33.554547   42383 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1212 13:11:33.704689   42383 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1212 13:11:33.772258   42383 out.go:204] üê≥  Preparing Kubernetes v1.26.3 on Docker 23.0.2 ...
I1212 13:11:33.773020   42383 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1212 13:11:33.796351   42383 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1212 13:11:33.800746   42383 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1212 13:11:33.814340   42383 preload.go:132] Checking if preload exists for k8s version v1.26.3 and runtime docker
I1212 13:11:33.814433   42383 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1212 13:11:33.842508   42383 docker.go:639] Got preloaded images: -- stdout --
budescode/django-docker:1.0.1
budescode/django-docker:1.0.0
mongo:latest
postgres:latest
django-app-v1:1.0.5
django-app-v1:1.0.1
django-app-v1:1.0.0
<none>:<none>
<none>:<none>
mongo:<none>
postgres:<none>
<none>:<none>
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/kube-apiserver:v1.26.3
registry.k8s.io/kube-scheduler:v1.26.3
registry.k8s.io/kube-controller-manager:v1.26.3
registry.k8s.io/kube-proxy:v1.26.3
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I1212 13:11:33.842525   42383 docker.go:569] Images already preloaded, skipping extraction
I1212 13:11:33.842590   42383 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1212 13:11:33.870949   42383 docker.go:639] Got preloaded images: -- stdout --
budescode/django-docker:1.0.1
budescode/django-docker:1.0.0
mongo:latest
postgres:latest
django-app-v1:1.0.5
django-app-v1:1.0.1
django-app-v1:1.0.0
<none>:<none>
<none>:<none>
mongo:<none>
postgres:<none>
<none>:<none>
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/kube-apiserver:v1.26.3
registry.k8s.io/kube-scheduler:v1.26.3
registry.k8s.io/kube-controller-manager:v1.26.3
registry.k8s.io/kube-proxy:v1.26.3
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I1212 13:11:33.870963   42383 cache_images.go:84] Images are preloaded, skipping loading
I1212 13:11:33.871027   42383 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1212 13:11:33.900023   42383 cni.go:84] Creating CNI manager for ""
I1212 13:11:33.900049   42383 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1212 13:11:33.900062   42383 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1212 13:11:33.900089   42383 kubeadm.go:172] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.26.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m]}
I1212 13:11:33.900299   42383 kubeadm.go:177] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.26.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1212 13:11:33.900412   42383 kubeadm.go:968] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.26.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.26.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1212 13:11:33.900494   42383 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.26.3
I1212 13:11:33.911698   42383 binaries.go:44] Found k8s binaries, skipping transfer
I1212 13:11:33.911758   42383 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1212 13:11:33.921616   42383 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1212 13:11:33.943055   42383 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1212 13:11:33.982365   42383 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2083 bytes)
I1212 13:11:33.996086   42383 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1212 13:11:33.998777   42383 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1212 13:11:34.008532   42383 certs.go:56] Setting up /home/budescode/.minikube/profiles/minikube for IP: 192.168.49.2
I1212 13:11:34.008546   42383 certs.go:186] acquiring lock for shared ca certs: {Name:mk768a0e42ba7cd8f898d26d304a57f810ee1f09 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 13:11:34.008666   42383 certs.go:195] skipping minikubeCA CA generation: /home/budescode/.minikube/ca.key
I1212 13:11:34.008997   42383 certs.go:195] skipping proxyClientCA CA generation: /home/budescode/.minikube/proxy-client-ca.key
I1212 13:11:34.009049   42383 certs.go:311] skipping minikube-user signed cert generation: /home/budescode/.minikube/profiles/minikube/client.key
I1212 13:11:34.009954   42383 certs.go:311] skipping minikube signed cert generation: /home/budescode/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1212 13:11:34.010245   42383 certs.go:311] skipping aggregator signed cert generation: /home/budescode/.minikube/profiles/minikube/proxy-client.key
I1212 13:11:34.010327   42383 certs.go:401] found cert: /home/budescode/.minikube/certs/home/budescode/.minikube/certs/ca-key.pem (1679 bytes)
I1212 13:11:34.010348   42383 certs.go:401] found cert: /home/budescode/.minikube/certs/home/budescode/.minikube/certs/ca.pem (1086 bytes)
I1212 13:11:34.010363   42383 certs.go:401] found cert: /home/budescode/.minikube/certs/home/budescode/.minikube/certs/cert.pem (1127 bytes)
I1212 13:11:34.010378   42383 certs.go:401] found cert: /home/budescode/.minikube/certs/home/budescode/.minikube/certs/key.pem (1679 bytes)
I1212 13:11:34.010788   42383 ssh_runner.go:362] scp /home/budescode/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1212 13:11:34.028511   42383 ssh_runner.go:362] scp /home/budescode/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1212 13:11:34.047906   42383 ssh_runner.go:362] scp /home/budescode/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1212 13:11:34.063254   42383 ssh_runner.go:362] scp /home/budescode/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1212 13:11:34.078840   42383 ssh_runner.go:362] scp /home/budescode/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1212 13:11:34.095997   42383 ssh_runner.go:362] scp /home/budescode/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1212 13:11:34.113984   42383 ssh_runner.go:362] scp /home/budescode/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1212 13:11:34.131326   42383 ssh_runner.go:362] scp /home/budescode/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1212 13:11:34.195223   42383 ssh_runner.go:362] scp /home/budescode/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1212 13:11:34.211651   42383 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1212 13:11:34.223942   42383 ssh_runner.go:195] Run: openssl version
I1212 13:11:34.232119   42383 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1212 13:11:34.240708   42383 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1212 13:11:34.243948   42383 certs.go:444] hashing: -rw-r--r-- 1 root root 1111 Jul 10 15:31 /usr/share/ca-certificates/minikubeCA.pem
I1212 13:11:34.243995   42383 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1212 13:11:34.248389   42383 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1212 13:11:34.255017   42383 kubeadm.go:401] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/budescode:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I1212 13:11:34.255100   42383 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1212 13:11:34.269226   42383 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1212 13:11:34.275915   42383 kubeadm.go:416] found existing configuration files, will attempt cluster restart
I1212 13:11:34.275921   42383 kubeadm.go:633] restartCluster start
I1212 13:11:34.275955   42383 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1212 13:11:34.281934   42383 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1212 13:11:34.283315   42383 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I1212 13:11:34.294713   42383 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1212 13:11:34.316744   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:34.316829   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:34.332934   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:34.833933   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:34.834079   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:34.863233   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:35.333636   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:35.333805   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:35.348250   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:35.833939   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:35.834092   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:35.863035   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:36.333894   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:36.334036   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:36.363350   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:36.833606   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:36.833792   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:36.862953   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:37.334137   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:37.334586   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:37.366667   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:37.833137   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:37.833339   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:37.862719   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:38.333874   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:38.334061   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:38.363464   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:38.833606   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:38.833804   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:38.864164   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:39.333165   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:39.333354   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:39.363095   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:39.833291   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:39.833369   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:39.843729   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:40.333444   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:40.333612   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:40.365080   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:40.833517   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:40.833658   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:40.863622   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:41.334012   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:41.334185   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:41.365737   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:41.833382   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:41.833601   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:41.869373   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:42.333019   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:42.333150   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:42.365412   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:42.833807   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:42.833990   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:42.865453   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:43.333019   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:43.333172   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:43.366055   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:43.834053   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:43.834251   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:43.864374   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:44.334055   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:44.334208   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:44.368160   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:44.368186   42383 api_server.go:165] Checking apiserver status ...
I1212 13:11:44.368351   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:11:44.392128   42383 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:11:44.392149   42383 kubeadm.go:608] needs reconfigure: apiserver error: timed out waiting for the condition
I1212 13:11:44.392156   42383 kubeadm.go:1120] stopping kube-system containers ...
I1212 13:11:44.392217   42383 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1212 13:11:44.415485   42383 docker.go:465] Stopping containers: [98d2a8147269 5720c7d55caa 62371e330264 3645c1793a75 e5305624239e 5857654fa981 c2621343f984 ef3dc081cbb2 8882b6c8ffa3 7fd9056fac54 18c00973c1ba ce014438722b 685a22f03a87 0aa0b075ea78 504b2e40d827 952a233d2f86 6eb895d900f2 7f9643d0d235 672d54f165e9 d152a507ad3c 7f54754fa17b 36ccf6ff2721 c5ba61782631 9b3adbeb556a 7d8f42a11d5b 3ed17d6f50c1 4cc4d2328080 4326ba4421e3 1f3166f45700 9a51cd83e543 eab523b85837]
I1212 13:11:44.415547   42383 ssh_runner.go:195] Run: docker stop 98d2a8147269 5720c7d55caa 62371e330264 3645c1793a75 e5305624239e 5857654fa981 c2621343f984 ef3dc081cbb2 8882b6c8ffa3 7fd9056fac54 18c00973c1ba ce014438722b 685a22f03a87 0aa0b075ea78 504b2e40d827 952a233d2f86 6eb895d900f2 7f9643d0d235 672d54f165e9 d152a507ad3c 7f54754fa17b 36ccf6ff2721 c5ba61782631 9b3adbeb556a 7d8f42a11d5b 3ed17d6f50c1 4cc4d2328080 4326ba4421e3 1f3166f45700 9a51cd83e543 eab523b85837
I1212 13:11:44.439694   42383 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1212 13:11:44.450887   42383 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1212 13:11:44.458264   42383 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Nov 24 12:33 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Dec 11 07:40 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Nov 24 12:33 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Dec 11 07:40 /etc/kubernetes/scheduler.conf

I1212 13:11:44.458303   42383 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1212 13:11:44.465060   42383 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1212 13:11:44.474091   42383 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1212 13:11:44.481217   42383 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1212 13:11:44.481279   42383 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1212 13:11:44.488888   42383 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1212 13:11:44.498509   42383 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1212 13:11:44.498558   42383 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1212 13:11:44.504583   42383 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1212 13:11:44.511267   42383 kubeadm.go:710] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1212 13:11:44.511278   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:11:44.796377   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:11:45.391970   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:11:45.593980   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:11:45.680566   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:11:45.720499   42383 api_server.go:51] waiting for apiserver process to appear ...
I1212 13:11:45.720579   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:11:46.230931   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:11:46.731697   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:11:47.231043   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:11:47.730318   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:11:47.748480   42383 api_server.go:71] duration metric: took 2.02797547s to wait for apiserver process to appear ...
I1212 13:11:47.748501   42383 api_server.go:87] waiting for apiserver healthz status ...
I1212 13:11:47.748514   42383 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1212 13:11:47.748983   42383 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1212 13:11:48.249605   42383 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1212 13:11:48.249991   42383 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1212 13:11:48.749674   42383 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1212 13:11:50.693465   42383 api_server.go:278] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1212 13:11:50.693479   42383 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1212 13:11:50.749970   42383 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1212 13:11:50.753654   42383 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1212 13:11:50.753665   42383 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1212 13:11:51.249435   42383 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1212 13:11:51.262125   42383 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1212 13:11:51.262229   42383 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1212 13:11:51.749348   42383 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1212 13:11:51.762947   42383 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1212 13:11:51.762987   42383 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1212 13:11:52.250140   42383 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1212 13:11:52.263219   42383 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I1212 13:11:52.292523   42383 api_server.go:140] control plane version: v1.26.3
I1212 13:11:52.292558   42383 api_server.go:130] duration metric: took 4.544042752s to wait for apiserver health ...
I1212 13:11:52.292581   42383 cni.go:84] Creating CNI manager for ""
I1212 13:11:52.292605   42383 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1212 13:11:52.297981   42383 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1212 13:11:52.300361   42383 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1212 13:11:52.323198   42383 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1212 13:11:52.357376   42383 system_pods.go:43] waiting for kube-system pods to appear ...
I1212 13:11:52.370575   42383 system_pods.go:59] 8 kube-system pods found
I1212 13:11:52.370597   42383 system_pods.go:61] "coredns-787d4945fb-p7gwk" [bc0b049c-5ba3-4234-9062-d964c2534c05] Running
I1212 13:11:52.370603   42383 system_pods.go:61] "etcd-minikube" [5ac3ba1f-31c4-4866-8287-76bab3a52e21] Running
I1212 13:11:52.370609   42383 system_pods.go:61] "kube-apiserver-minikube" [4696deba-e9c6-46f2-8742-d948acee4342] Running
I1212 13:11:52.370614   42383 system_pods.go:61] "kube-controller-manager-minikube" [094d2859-43cf-4d2e-b0a3-4876382cd92e] Running
I1212 13:11:52.370618   42383 system_pods.go:61] "kube-proxy-ngd7w" [80dd6f9d-4419-45cb-b5d3-bc1c38f52b36] Running
I1212 13:11:52.370624   42383 system_pods.go:61] "kube-scheduler-minikube" [1b87ff6a-0d98-44a0-8a09-0c9380c495b0] Running
I1212 13:11:52.370632   42383 system_pods.go:61] "metrics-server-6588d95b98-htvx8" [4f238623-0346-4ab0-a12f-cc98809d30fb] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I1212 13:11:52.370638   42383 system_pods.go:61] "storage-provisioner" [3ba0f71a-accb-4781-82cc-4e54574b6bc0] Running
I1212 13:11:52.370644   42383 system_pods.go:74] duration metric: took 13.256558ms to wait for pod list to return data ...
I1212 13:11:52.370650   42383 node_conditions.go:102] verifying NodePressure condition ...
I1212 13:11:52.373589   42383 node_conditions.go:122] node storage ephemeral capacity is 490617784Ki
I1212 13:11:52.373607   42383 node_conditions.go:123] node cpu capacity is 8
I1212 13:11:52.373616   42383 node_conditions.go:105] duration metric: took 2.962029ms to run NodePressure ...
I1212 13:11:52.373630   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:11:52.547351   42383 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1212 13:11:52.554101   42383 ops.go:34] apiserver oom_adj: -16
I1212 13:11:52.554111   42383 kubeadm.go:637] restartCluster took 18.278186133s
I1212 13:11:52.554115   42383 kubeadm.go:403] StartCluster complete in 18.299103711s
I1212 13:11:52.554125   42383 settings.go:142] acquiring lock: {Name:mkeb7888c5ad6d44dbae3936e889abfed472f139 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 13:11:52.554205   42383 settings.go:150] Updating kubeconfig:  /home/budescode/.kube/config
I1212 13:11:52.555035   42383 lock.go:35] WriteFile acquiring /home/budescode/.kube/config: {Name:mk19bfd7b8f5c751e2821e42f4fd60858ed70b62 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 13:11:52.555204   42383 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1212 13:11:52.555319   42383 addons.go:496] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1212 13:11:52.555392   42383 addons.go:66] Setting storage-provisioner=true in profile "minikube"
I1212 13:11:52.555415   42383 addons.go:228] Setting addon storage-provisioner=true in "minikube"
W1212 13:11:52.555421   42383 addons.go:237] addon storage-provisioner should already be in state true
I1212 13:11:52.555428   42383 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I1212 13:11:52.555451   42383 addons.go:66] Setting default-storageclass=true in profile "minikube"
I1212 13:11:52.555460   42383 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1212 13:11:52.555461   42383 host.go:66] Checking if "minikube" exists ...
I1212 13:11:52.555578   42383 addons.go:66] Setting metrics-server=true in profile "minikube"
I1212 13:11:52.555596   42383 addons.go:228] Setting addon metrics-server=true in "minikube"
W1212 13:11:52.555604   42383 addons.go:237] addon metrics-server should already be in state true
I1212 13:11:52.555648   42383 host.go:66] Checking if "minikube" exists ...
I1212 13:11:52.555695   42383 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:11:52.555891   42383 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:11:52.556057   42383 addons.go:66] Setting dashboard=true in profile "minikube"
I1212 13:11:52.556101   42383 addons.go:228] Setting addon dashboard=true in "minikube"
I1212 13:11:52.556107   42383 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1212 13:11:52.556111   42383 addons.go:237] addon dashboard should already be in state true
I1212 13:11:52.556151   42383 host.go:66] Checking if "minikube" exists ...
I1212 13:11:52.556593   42383 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:11:52.560103   42383 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1212 13:11:52.560134   42383 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1212 13:11:52.562934   42383 out.go:177] üîé  Verifying Kubernetes components...
I1212 13:11:52.564703   42383 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1212 13:11:52.583231   42383 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1212 13:11:52.582245   42383 addons.go:228] Setting addon default-storageclass=true in "minikube"
I1212 13:11:52.587329   42383 addons.go:420] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1212 13:11:52.588027   42383 out.go:177]     ‚ñ™ Using image registry.k8s.io/metrics-server/metrics-server:v0.6.3
I1212 13:11:52.591316   42383 addons.go:420] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I1212 13:11:52.591328   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
W1212 13:11:52.588437   42383 addons.go:237] addon default-storageclass should already be in state true
I1212 13:11:52.591363   42383 host.go:66] Checking if "minikube" exists ...
I1212 13:11:52.588439   42383 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I1212 13:11:52.591398   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:52.588457   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1212 13:11:52.591460   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:52.591742   42383 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:11:52.598368   42383 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1212 13:11:52.601181   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1212 13:11:52.601194   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1212 13:11:52.601291   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:52.608280   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:52.615545   42383 addons.go:420] installing /etc/kubernetes/addons/storageclass.yaml
I1212 13:11:52.615558   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1212 13:11:52.615605   42383 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:11:52.618028   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:52.620660   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:52.632352   42383 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/budescode/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:11:52.726376   42383 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1212 13:11:52.727159   42383 addons.go:420] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I1212 13:11:52.727171   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I1212 13:11:52.727917   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1212 13:11:52.727930   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1212 13:11:52.738776   42383 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1212 13:11:52.747727   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1212 13:11:52.747738   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1212 13:11:52.747819   42383 addons.go:420] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I1212 13:11:52.747827   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I1212 13:11:52.763293   42383 addons.go:420] installing /etc/kubernetes/addons/metrics-server-service.yaml
I1212 13:11:52.763293   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1212 13:11:52.763305   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I1212 13:11:52.763305   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1212 13:11:52.779024   42383 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I1212 13:11:52.779048   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1212 13:11:52.779055   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1212 13:11:52.796664   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-role.yaml
I1212 13:11:52.796678   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1212 13:11:52.812595   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1212 13:11:52.812608   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1212 13:11:52.834790   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1212 13:11:52.834806   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1212 13:11:52.850233   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1212 13:11:52.850243   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1212 13:11:52.878713   42383 addons.go:420] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1212 13:11:52.878725   42383 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1212 13:11:52.907740   42383 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1212 13:11:57.997994   42383 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (5.442769186s)
I1212 13:11:57.998059   42383 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (5.433328303s)
I1212 13:11:57.998086   42383 start.go:889] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1212 13:11:57.998101   42383 api_server.go:51] waiting for apiserver process to appear ...
I1212 13:11:57.998170   42383 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:11:58.601571   42383 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.875171957s)
I1212 13:11:58.601626   42383 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (5.862833833s)
I1212 13:11:58.615456   42383 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (5.836406514s)
I1212 13:11:58.615473   42383 addons.go:464] Verifying addon metrics-server=true in "minikube"
I1212 13:11:58.632443   42383 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (5.724678087s)
I1212 13:11:58.632455   42383 api_server.go:71] duration metric: took 6.072298432s to wait for apiserver process to appear ...
I1212 13:11:58.632461   42383 api_server.go:87] waiting for apiserver healthz status ...
I1212 13:11:58.632470   42383 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1212 13:11:58.634265   42383 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1212 13:11:58.636174   42383 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, metrics-server, dashboard
I1212 13:11:58.638685   42383 addons.go:499] enable addons completed in 6.083375442s: enabled=[storage-provisioner default-storageclass metrics-server dashboard]
I1212 13:11:58.636686   42383 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I1212 13:11:58.639220   42383 api_server.go:140] control plane version: v1.26.3
I1212 13:11:58.639227   42383 api_server.go:130] duration metric: took 6.762144ms to wait for apiserver health ...
I1212 13:11:58.639231   42383 system_pods.go:43] waiting for kube-system pods to appear ...
I1212 13:11:58.643526   42383 system_pods.go:59] 8 kube-system pods found
I1212 13:11:58.643539   42383 system_pods.go:61] "coredns-787d4945fb-p7gwk" [bc0b049c-5ba3-4234-9062-d964c2534c05] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1212 13:11:58.643545   42383 system_pods.go:61] "etcd-minikube" [5ac3ba1f-31c4-4866-8287-76bab3a52e21] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1212 13:11:58.643550   42383 system_pods.go:61] "kube-apiserver-minikube" [4696deba-e9c6-46f2-8742-d948acee4342] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1212 13:11:58.643555   42383 system_pods.go:61] "kube-controller-manager-minikube" [094d2859-43cf-4d2e-b0a3-4876382cd92e] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1212 13:11:58.643559   42383 system_pods.go:61] "kube-proxy-ngd7w" [80dd6f9d-4419-45cb-b5d3-bc1c38f52b36] Running
I1212 13:11:58.643563   42383 system_pods.go:61] "kube-scheduler-minikube" [1b87ff6a-0d98-44a0-8a09-0c9380c495b0] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1212 13:11:58.643568   42383 system_pods.go:61] "metrics-server-6588d95b98-htvx8" [4f238623-0346-4ab0-a12f-cc98809d30fb] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I1212 13:11:58.643571   42383 system_pods.go:61] "storage-provisioner" [3ba0f71a-accb-4781-82cc-4e54574b6bc0] Running
I1212 13:11:58.643575   42383 system_pods.go:74] duration metric: took 4.341644ms to wait for pod list to return data ...
I1212 13:11:58.643581   42383 kubeadm.go:578] duration metric: took 6.083425855s to wait for : map[apiserver:true system_pods:true] ...
I1212 13:11:58.643589   42383 node_conditions.go:102] verifying NodePressure condition ...
I1212 13:11:58.645209   42383 node_conditions.go:122] node storage ephemeral capacity is 490617784Ki
I1212 13:11:58.645218   42383 node_conditions.go:123] node cpu capacity is 8
I1212 13:11:58.645225   42383 node_conditions.go:105] duration metric: took 1.633357ms to run NodePressure ...
I1212 13:11:58.645253   42383 start.go:228] waiting for startup goroutines ...
I1212 13:11:58.645260   42383 start.go:233] waiting for cluster config update ...
I1212 13:11:58.645268   42383 start.go:242] writing updated cluster config ...
I1212 13:11:58.645517   42383 ssh_runner.go:195] Run: rm -f paused
I1212 13:11:58.686210   42383 start.go:568] kubectl: 1.27.3, cluster: 1.26.3 (minor skew: 1)
I1212 13:11:58.689931   42383 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Tue 2023-12-12 12:11:23 UTC, end at Tue 2023-12-12 12:30:37 UTC. --
Dec 12 12:11:33 minikube cri-dockerd[1218]: time="2023-12-12T12:11:33Z" level=info msg="Loaded network plugin cni"
Dec 12 12:11:33 minikube cri-dockerd[1218]: time="2023-12-12T12:11:33Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 12 12:11:33 minikube cri-dockerd[1218]: time="2023-12-12T12:11:33Z" level=info msg="Docker Info: &{ID:601ab9fa-7ad6-481d-a05b-7edc6409f188 Containers:46 ContainersRunning:0 ContainersPaused:0 ContainersStopped:46 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:24 OomKillDisable:false NGoroutines:33 SystemTime:2023-12-12T12:11:33.308346299Z LoggingDriver:json-file CgroupDriver:systemd CgroupVersion:2 NEventsListener:0 KernelVersion:6.2.0-37-generic OperatingSystem:Ubuntu 20.04.5 LTS OSVersion:20.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0005b1e30 NCPU:8 MemTotal:16568668160 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:23.0.2 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID:v1.1.5-0-gf19387a Expected:v1.1.5-0-gf19387a} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Dec 12 12:11:33 minikube cri-dockerd[1218]: time="2023-12-12T12:11:33Z" level=info msg="Setting cgroupDriver systemd"
Dec 12 12:11:33 minikube cri-dockerd[1218]: time="2023-12-12T12:11:33Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 12 12:11:33 minikube cri-dockerd[1218]: time="2023-12-12T12:11:33Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 12 12:11:33 minikube cri-dockerd[1218]: time="2023-12-12T12:11:33Z" level=info msg="Start cri-dockerd grpc backend"
Dec 12 12:11:33 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5c6664855-hxh7v_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b23e7fdd7b0b572ce97c7ea61266efd9f36de89bec3fb791c98851aa443e6283\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5c6664855-hxh7v_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d5eb9ebf1f0e05c1c0bf60a9f28d74a75bac8aabfea9cfbcf46402a4b4019d5e\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"metrics-server-6588d95b98-htvx8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"18c00973c1bae99e091b233ef5356da475233a2078973c89d97001154b897d44\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"django-deployment-6c4bb6586c-jw5bv_django-app-ns\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c939f023cb38e57519413bb8a597e4f9aa72f517ec0e8bc03934a68b5bc7160f\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"django-deployment-6c4bb6586c-jw5bv_django-app-ns\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8a6d90732cb80c145a668fa9cd74483c89ee4db06d96d13a7099ca505fb6a328\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-787d4945fb-p7gwk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7fd9056fac547447a9209106f53d122592e9509d24c501dc292de16687ca8b22\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-787d4945fb-p7gwk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c5ba61782631f90697ceab7a6f9dc5018e7da0f7c82d8e6ff696ee5ff6408228\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"django-deployment-dd6b98f7c-gdh2k_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0ad0329992ce5233e5682581d60145199eb4f3e5a660037d259076e2f167a23e\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"django-deployment-dd6b98f7c-gdh2k_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b0e5172a87cdd1404757e730381c490f9f01515f6c312f110734de0a3193465c\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-55c4cbbc7c-b6d94_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cd969aaf13ef4877767c324523e6d66a3bc72f7ea7e782bc794d4ed8e9597cc6\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-55c4cbbc7c-b6d94_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"433573308c51ec989cac6589aacb12ef2e92eeeeee3b8ddd5602eea3a6c82f03\""
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"672d54f165e9961c539e44d515c2ee474c78431f7059669b70b4824c8eb0ca2b\". Proceed without further sandbox information."
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"36ccf6ff272122d593f42744682a3b8407e341c4253cda5e12ef7b0d4a66a4e9\". Proceed without further sandbox information."
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"4cc4d23280808911a895045eeca737ceb32384e853b0d48e99758b26442a81eb\". Proceed without further sandbox information."
Dec 12 12:11:46 minikube cri-dockerd[1218]: time="2023-12-12T12:11:46Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"1f3166f45700ef2cffc24c9faa49d0a30a993d22408a171e4e5e185cdcc7d958\". Proceed without further sandbox information."
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e3947b31d077c013e3dd46774760430673ce9d51fd24c75639a6d9dbf4b43fc2/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c28dd5dba94e479176a5f3a372c97b81d4282388fa051f901eb6a1ce381669ef/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e78f3348f0aec575c28d92f6f53cc3bb6575b032133f62239877ebb5be7d00d2/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ba287ecbd21a4cfb31fbc8bf91f079ded3928da927a93b9c41b5e48d2bcf2421/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-55c4cbbc7c-b6d94_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cd969aaf13ef4877767c324523e6d66a3bc72f7ea7e782bc794d4ed8e9597cc6\""
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"django-deployment-dd6b98f7c-gdh2k_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0ad0329992ce5233e5682581d60145199eb4f3e5a660037d259076e2f167a23e\""
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5c6664855-hxh7v_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b23e7fdd7b0b572ce97c7ea61266efd9f36de89bec3fb791c98851aa443e6283\""
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-787d4945fb-p7gwk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7fd9056fac547447a9209106f53d122592e9509d24c501dc292de16687ca8b22\""
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-787d4945fb-p7gwk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c5ba61782631f90697ceab7a6f9dc5018e7da0f7c82d8e6ff696ee5ff6408228\""
Dec 12 12:11:47 minikube cri-dockerd[1218]: time="2023-12-12T12:11:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"metrics-server-6588d95b98-htvx8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"18c00973c1bae99e091b233ef5356da475233a2078973c89d97001154b897d44\""
Dec 12 12:11:50 minikube cri-dockerd[1218]: time="2023-12-12T12:11:50Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 12 12:11:52 minikube cri-dockerd[1218]: time="2023-12-12T12:11:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a269d425b5d187955995a5ef24fbce4ab447c552a7f29df92af9381c538b137/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Dec 12 12:11:52 minikube cri-dockerd[1218]: time="2023-12-12T12:11:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c9f7c3625a9b99fa4b1f6583ca7c0ae7e7cfed1e8929d5214ebc469986da867c/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Dec 12 12:11:53 minikube cri-dockerd[1218]: time="2023-12-12T12:11:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/14bb25ce66266e24a631b97fc185a281db4a489e667b809e132884882d0e7249/resolv.conf as [nameserver 10.96.0.10 search django-app-ns.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 12:11:53 minikube cri-dockerd[1218]: time="2023-12-12T12:11:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7b5c4f45f0552de4d55d58cabc84e04153d591f85570c7b0755dba57b896afa5/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Dec 12 12:11:53 minikube cri-dockerd[1218]: time="2023-12-12T12:11:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4a61e5c85a2b97fd21fb4db70454fff744cab2042fcfb7a17c0b344a6cab53fd/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 12:11:54 minikube cri-dockerd[1218]: time="2023-12-12T12:11:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/80edd2d6e3f6c3170450d4724dcadd5ad9c87efc28c454c32f201c31a39d2dfa/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 12:11:54 minikube cri-dockerd[1218]: time="2023-12-12T12:11:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ca09e1f6ebd00a32e30dc5a727e20822fecbec57917e2605d693fd234fd09018/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 12:11:54 minikube cri-dockerd[1218]: time="2023-12-12T12:11:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/94a59b0cc242b9bc7de5b8997bc1eebc2ad495eb270a38b06968b75ea23a8129/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 12:12:01 minikube dockerd[963]: time="2023-12-12T12:12:01.001431816Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 12:12:01 minikube dockerd[963]: time="2023-12-12T12:12:01.001512698Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 12:12:03 minikube cri-dockerd[1218]: time="2023-12-12T12:12:03Z" level=info msg="Stop pulling image budescode/django-docker:1.0.1: Status: Image is up to date for budescode/django-docker:1.0.1"
Dec 12 12:12:15 minikube dockerd[963]: time="2023-12-12T12:12:15.517163325Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 12:12:15 minikube dockerd[963]: time="2023-12-12T12:12:15.517298340Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 12:12:23 minikube dockerd[963]: time="2023-12-12T12:12:23.543617873Z" level=info msg="ignoring event" container=bc16084884c47bdd3e5fda61f9a3987956f4fafe0572495f61014852fa0a6a2e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 12 12:12:47 minikube dockerd[963]: time="2023-12-12T12:12:47.930640920Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 12:12:47 minikube dockerd[963]: time="2023-12-12T12:12:47.930740183Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 12:13:37 minikube dockerd[963]: time="2023-12-12T12:13:37.828678518Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 12:13:37 minikube dockerd[963]: time="2023-12-12T12:13:37.828765797Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 12:15:16 minikube dockerd[963]: time="2023-12-12T12:15:16.150600557Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 12:15:16 minikube dockerd[963]: time="2023-12-12T12:15:16.150690650Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 12:18:13 minikube dockerd[963]: time="2023-12-12T12:18:13.098593590Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 12:18:13 minikube dockerd[963]: time="2023-12-12T12:18:13.098679033Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 12:23:29 minikube dockerd[963]: time="2023-12-12T12:23:29.050011623Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 12:23:29 minikube dockerd[963]: time="2023-12-12T12:23:29.050086801Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 12:28:38 minikube dockerd[963]: time="2023-12-12T12:28:38.754060169Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 12:28:38 minikube dockerd[963]: time="2023-12-12T12:28:38.754165578Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                             CREATED             STATE               NAME                        ATTEMPT             POD ID
b9ef81c4b8f90       6e38f40d628db                                                                                     17 minutes ago      Running             storage-provisioner         17                  c9f7c3625a9b9
a9650ae244803       budescode/django-docker@sha256:b6dc3962bc71f6b29636dc43c29bc8ea24a697066ad5b5e0c0c68ccfe68340c3   18 minutes ago      Running             django-container            3                   80edd2d6e3f6c
b8d4aecc5279b       07655ddf2eebe                                                                                     18 minutes ago      Running             kubernetes-dashboard        15                  94a59b0cc242b
f11d22914789e       817bbe3f2e517                                                                                     18 minutes ago      Running             metrics-server              13                  ca09e1f6ebd00
66502ba79b8f4       115053965e86b                                                                                     18 minutes ago      Running             dashboard-metrics-scraper   9                   4a61e5c85a2b9
803a1eb6c8aec       92ed2bec97a63                                                                                     18 minutes ago      Running             kube-proxy                  9                   7b5c4f45f0552
bc16084884c47       6e38f40d628db                                                                                     18 minutes ago      Exited              storage-provisioner         16                  c9f7c3625a9b9
4c6551523af02       5185b96f0becf                                                                                     18 minutes ago      Running             coredns                     11                  2a269d425b5d1
8f339a025a832       1d9b3cbae03ce                                                                                     18 minutes ago      Running             kube-apiserver              18                  e78f3348f0aec
2bb7940864640       5a79047369329                                                                                     18 minutes ago      Running             kube-scheduler              18                  c28dd5dba94e4
3e33f2f648891       fce326961ae2d                                                                                     18 minutes ago      Running             etcd                        18                  ba287ecbd21a4
adb9a38bb9fb0       ce8c2293ef09c                                                                                     18 minutes ago      Running             kube-controller-manager     18                  e3947b31d077c
af297267a1e9d       budescode/django-docker@sha256:b6dc3962bc71f6b29636dc43c29bc8ea24a697066ad5b5e0c0c68ccfe68340c3   27 hours ago        Exited              django-container            2                   0ad0329992ce5
98d2a81472697       817bbe3f2e517                                                                                     27 hours ago        Exited              metrics-server              12                  18c00973c1bae
9cf5ba7b1b84d       115053965e86b                                                                                     27 hours ago        Exited              dashboard-metrics-scraper   8                   b23e7fdd7b0b5
62371e3302646       5185b96f0becf                                                                                     27 hours ago        Exited              coredns                     10                  7fd9056fac547
a7511e58b96ad       07655ddf2eebe                                                                                     27 hours ago        Exited              kubernetes-dashboard        14                  cd969aaf13ef4
e5305624239e3       ce8c2293ef09c                                                                                     27 hours ago        Exited              kube-controller-manager     17                  685a22f03a877
5857654fa9810       5a79047369329                                                                                     27 hours ago        Exited              kube-scheduler              17                  6eb895d900f28
c2621343f9842       92ed2bec97a63                                                                                     27 hours ago        Exited              kube-proxy                  8                   0aa0b075ea784
ef3dc081cbb2b       fce326961ae2d                                                                                     27 hours ago        Exited              etcd                        17                  952a233d2f868
8882b6c8ffa33       1d9b3cbae03ce                                                                                     27 hours ago        Exited              kube-apiserver              17                  504b2e40d8277
9620957d156cb       92aaf54799b84                                                                                     2 weeks ago         Exited              django-container            1                   8a6d90732cb80

* 
* ==> coredns [4c6551523af0] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:59736 - 33460 "HINFO IN 4501793247430912031.868470042317355206. udp 56 false 512" NXDOMAIN qr,rd,ra 131 2.079350773s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:38344 - 15804 "HINFO IN 4501793247430912031.868470042317355206. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 131 0.000323062s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> coredns [62371e330264] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:39299 - 12192 "HINFO IN 6900858147681303261.6608435841533072577. udp 57 false 512" NXDOMAIN qr,rd,ra 132 2.16684803s
[INFO] 127.0.0.1:35910 - 1569 "HINFO IN 6900858147681303261.6608435841533072577. udp 57 false 512" NXDOMAIN qr,aa,rd,ra 132 0.000329436s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 20 Jul 2023 08:27:58 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 12 Dec 2023 12:30:35 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 12 Dec 2023 12:27:10 +0000   Thu, 20 Jul 2023 08:27:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 12 Dec 2023 12:27:10 +0000   Thu, 20 Jul 2023 08:27:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 12 Dec 2023 12:27:10 +0000   Thu, 20 Jul 2023 08:27:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 12 Dec 2023 12:27:10 +0000   Thu, 20 Jul 2023 08:27:58 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  490617784Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16180340Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  490617784Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16180340Ki
  pods:               110
System Info:
  Machine ID:                 4be7b89512914632b7eb285a3ba7704a
  System UUID:                2a813c24-800f-4a60-aa3a-b3d051f29fd5
  Boot ID:                    4514455a-1081-4b8e-af95-b361d6e6deb1
  Kernel Version:             6.2.0-37-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://23.0.2
  Kubelet Version:            v1.26.3
  Kube-Proxy Version:         v1.26.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     django-deployment-dd6b98f7c-gdh2k            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d23h
  django-app-ns               django-deployment-6c4bb6586c-jw5bv           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20d
  kube-system                 coredns-787d4945fb-p7gwk                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     145d
  kube-system                 etcd-minikube                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         145d
  kube-system                 kube-apiserver-minikube                      250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         145d
  kube-system                 kube-controller-manager-minikube             200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         145d
  kube-system                 kube-proxy-ngd7w                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         145d
  kube-system                 kube-scheduler-minikube                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         145d
  kube-system                 metrics-server-6588d95b98-htvx8              100m (1%!)(MISSING)     0 (0%!)(MISSING)      200Mi (1%!)(MISSING)       0 (0%!)(MISSING)         145d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         145d
  kubernetes-dashboard        dashboard-metrics-scraper-5c6664855-hxh7v    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         145d
  kubernetes-dashboard        kubernetes-dashboard-55c4cbbc7c-b6d94        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         145d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%!)(MISSING)  0 (0%!)(MISSING)
  memory             370Mi (2%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 26h                kube-proxy       
  Normal  Starting                 18m                kube-proxy       
  Normal  RegisteredNode           26h                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 18m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  18m (x9 over 18m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    18m (x7 over 18m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     18m (x7 over 18m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  18m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           18m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Dec12 12:17] kauditd_printk_skb: 88 callbacks suppressed
[Dec12 12:18] kauditd_printk_skb: 116 callbacks suppressed
[Dec12 12:19] kauditd_printk_skb: 88 callbacks suppressed
[Dec12 12:20] kauditd_printk_skb: 128 callbacks suppressed
[Dec12 12:21] kauditd_printk_skb: 78 callbacks suppressed
[  +5.929210] kauditd_printk_skb: 33 callbacks suppressed
[Dec12 12:22] kauditd_printk_skb: 87 callbacks suppressed
[Dec12 12:23] kauditd_printk_skb: 116 callbacks suppressed
[Dec12 12:24] kauditd_printk_skb: 87 callbacks suppressed
[Dec12 12:25] kauditd_printk_skb: 102 callbacks suppressed
[Dec12 12:26] kauditd_printk_skb: 101 callbacks suppressed
[Dec12 12:27] kauditd_printk_skb: 142 callbacks suppressed
[Dec12 12:28] kauditd_printk_skb: 114 callbacks suppressed
[Dec12 12:29] kauditd_printk_skb: 104 callbacks suppressed
[  +5.771732] kauditd_printk_skb: 7 callbacks suppressed

* 
* ==> etcd [3e33f2f64889] <==
* {"level":"info","ts":"2023-12-12T12:26:26.512Z","caller":"traceutil/trace.go:171","msg":"trace[2040200998] transaction","detail":"{read_only:false; response_revision:118491; number_of_response:1; }","duration":"110.717585ms","start":"2023-12-12T12:26:26.401Z","end":"2023-12-12T12:26:26.512Z","steps":["trace[2040200998] 'process raft request'  (duration: 110.646371ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:26:36.716Z","caller":"traceutil/trace.go:171","msg":"trace[1614300643] transaction","detail":"{read_only:false; response_revision:118499; number_of_response:1; }","duration":"110.960311ms","start":"2023-12-12T12:26:36.605Z","end":"2023-12-12T12:26:36.716Z","steps":["trace[1614300643] 'process raft request'  (duration: 110.789077ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:26:48.946Z","caller":"traceutil/trace.go:171","msg":"trace[1675556665] transaction","detail":"{read_only:false; response_revision:118508; number_of_response:1; }","duration":"111.115021ms","start":"2023-12-12T12:26:48.834Z","end":"2023-12-12T12:26:48.946Z","steps":["trace[1675556665] 'process raft request'  (duration: 110.881445ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:26:49.566Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":118269}
{"level":"info","ts":"2023-12-12T12:26:49.569Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":118269,"took":"1.936194ms","hash":791566723}
{"level":"info","ts":"2023-12-12T12:26:49.569Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":791566723,"revision":118269,"compact-revision":118030}
{"level":"info","ts":"2023-12-12T12:27:01.140Z","caller":"traceutil/trace.go:171","msg":"trace[1145136528] transaction","detail":"{read_only:false; response_revision:118518; number_of_response:1; }","duration":"111.010368ms","start":"2023-12-12T12:27:01.029Z","end":"2023-12-12T12:27:01.140Z","steps":["trace[1145136528] 'process raft request'  (duration: 110.695094ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:05.302Z","caller":"traceutil/trace.go:171","msg":"trace[1994129523] transaction","detail":"{read_only:false; response_revision:118523; number_of_response:1; }","duration":"114.674889ms","start":"2023-12-12T12:27:05.188Z","end":"2023-12-12T12:27:05.302Z","steps":["trace[1994129523] 'process raft request'  (duration: 114.334458ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:10.938Z","caller":"traceutil/trace.go:171","msg":"trace[2036265288] transaction","detail":"{read_only:false; response_revision:118526; number_of_response:1; }","duration":"123.278297ms","start":"2023-12-12T12:27:10.815Z","end":"2023-12-12T12:27:10.938Z","steps":["trace[2036265288] 'process raft request'  (duration: 122.992492ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:19.558Z","caller":"traceutil/trace.go:171","msg":"trace[1740991282] transaction","detail":"{read_only:false; response_revision:118535; number_of_response:1; }","duration":"110.995703ms","start":"2023-12-12T12:27:19.447Z","end":"2023-12-12T12:27:19.558Z","steps":["trace[1740991282] 'process raft request'  (duration: 110.729622ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:29.758Z","caller":"traceutil/trace.go:171","msg":"trace[1524664202] transaction","detail":"{read_only:false; response_revision:118543; number_of_response:1; }","duration":"111.172218ms","start":"2023-12-12T12:27:29.647Z","end":"2023-12-12T12:27:29.758Z","steps":["trace[1524664202] 'process raft request'  (duration: 110.806094ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:31.582Z","caller":"traceutil/trace.go:171","msg":"trace[1128023800] transaction","detail":"{read_only:false; response_revision:118544; number_of_response:1; }","duration":"113.032315ms","start":"2023-12-12T12:27:31.469Z","end":"2023-12-12T12:27:31.582Z","steps":["trace[1128023800] 'process raft request'  (duration: 112.821088ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:35.925Z","caller":"traceutil/trace.go:171","msg":"trace[844396042] transaction","detail":"{read_only:false; response_revision:118549; number_of_response:1; }","duration":"111.080105ms","start":"2023-12-12T12:27:35.814Z","end":"2023-12-12T12:27:35.925Z","steps":["trace[844396042] 'process raft request'  (duration: 110.861819ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:44.169Z","caller":"traceutil/trace.go:171","msg":"trace[2114146053] transaction","detail":"{read_only:false; response_revision:118556; number_of_response:1; }","duration":"112.52404ms","start":"2023-12-12T12:27:44.057Z","end":"2023-12-12T12:27:44.169Z","steps":["trace[2114146053] 'process raft request'  (duration: 112.345643ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:50.336Z","caller":"traceutil/trace.go:171","msg":"trace[235740285] transaction","detail":"{read_only:false; response_revision:118559; number_of_response:1; }","duration":"112.666493ms","start":"2023-12-12T12:27:50.223Z","end":"2023-12-12T12:27:50.336Z","steps":["trace[235740285] 'process raft request'  (duration: 112.392029ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:52.016Z","caller":"traceutil/trace.go:171","msg":"trace[1477917223] transaction","detail":"{read_only:false; response_revision:118560; number_of_response:1; }","duration":"110.968083ms","start":"2023-12-12T12:27:51.905Z","end":"2023-12-12T12:27:52.016Z","steps":["trace[1477917223] 'process raft request'  (duration: 110.735437ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:27:56.506Z","caller":"traceutil/trace.go:171","msg":"trace[1594209904] linearizableReadLoop","detail":"{readStateIndex:147787; appliedIndex:147786; }","duration":"109.066319ms","start":"2023-12-12T12:27:56.397Z","end":"2023-12-12T12:27:56.506Z","steps":["trace[1594209904] 'read index received'  (duration: 108.816289ms)","trace[1594209904] 'applied index is now lower than readState.Index'  (duration: 248.536¬µs)"],"step_count":2}
{"level":"info","ts":"2023-12-12T12:27:56.506Z","caller":"traceutil/trace.go:171","msg":"trace[1371768846] transaction","detail":"{read_only:false; response_revision:118565; number_of_response:1; }","duration":"111.539314ms","start":"2023-12-12T12:27:56.395Z","end":"2023-12-12T12:27:56.506Z","steps":["trace[1371768846] 'process raft request'  (duration: 111.179581ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-12T12:27:56.507Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"109.314087ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-12T12:27:56.507Z","caller":"traceutil/trace.go:171","msg":"trace[2088925626] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:118565; }","duration":"109.448308ms","start":"2023-12-12T12:27:56.397Z","end":"2023-12-12T12:27:56.507Z","steps":["trace[2088925626] 'agreement among raft nodes before linearized reading'  (duration: 109.25562ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:00.648Z","caller":"traceutil/trace.go:171","msg":"trace[2124817697] transaction","detail":"{read_only:false; response_revision:118567; number_of_response:1; }","duration":"111.118102ms","start":"2023-12-12T12:28:00.537Z","end":"2023-12-12T12:28:00.648Z","steps":["trace[2124817697] 'process raft request'  (duration: 110.832123ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:06.817Z","caller":"traceutil/trace.go:171","msg":"trace[1569689191] transaction","detail":"{read_only:false; response_revision:118573; number_of_response:1; }","duration":"111.073869ms","start":"2023-12-12T12:28:06.705Z","end":"2023-12-12T12:28:06.817Z","steps":["trace[1569689191] 'process raft request'  (duration: 110.882523ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:08.944Z","caller":"traceutil/trace.go:171","msg":"trace[622283905] transaction","detail":"{read_only:false; response_revision:118574; number_of_response:1; }","duration":"116.209286ms","start":"2023-12-12T12:28:08.828Z","end":"2023-12-12T12:28:08.944Z","steps":["trace[622283905] 'process raft request'  (duration: 115.978773ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:11.069Z","caller":"traceutil/trace.go:171","msg":"trace[543362935] transaction","detail":"{read_only:false; response_revision:118575; number_of_response:1; }","duration":"111.153149ms","start":"2023-12-12T12:28:10.958Z","end":"2023-12-12T12:28:11.069Z","steps":["trace[543362935] 'process raft request'  (duration: 110.937302ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:19.258Z","caller":"traceutil/trace.go:171","msg":"trace[1145719575] transaction","detail":"{read_only:false; response_revision:118582; number_of_response:1; }","duration":"111.196913ms","start":"2023-12-12T12:28:19.147Z","end":"2023-12-12T12:28:19.258Z","steps":["trace[1145719575] 'process raft request'  (duration: 110.882395ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:21.383Z","caller":"traceutil/trace.go:171","msg":"trace[105640873] transaction","detail":"{read_only:false; response_revision:118583; number_of_response:1; }","duration":"111.160514ms","start":"2023-12-12T12:28:21.272Z","end":"2023-12-12T12:28:21.383Z","steps":["trace[105640873] 'process raft request'  (duration: 110.776566ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:31.642Z","caller":"traceutil/trace.go:171","msg":"trace[2140977095] transaction","detail":"{read_only:false; response_revision:118591; number_of_response:1; }","duration":"112.744178ms","start":"2023-12-12T12:28:31.529Z","end":"2023-12-12T12:28:31.642Z","steps":["trace[2140977095] 'process raft request'  (duration: 112.577227ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:45.947Z","caller":"traceutil/trace.go:171","msg":"trace[1735285574] transaction","detail":"{read_only:false; response_revision:118604; number_of_response:1; }","duration":"111.205915ms","start":"2023-12-12T12:28:45.836Z","end":"2023-12-12T12:28:45.947Z","steps":["trace[1735285574] 'process raft request'  (duration: 110.822578ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:28:52.183Z","caller":"traceutil/trace.go:171","msg":"trace[1111605039] transaction","detail":"{read_only:false; response_revision:118607; number_of_response:1; }","duration":"111.228046ms","start":"2023-12-12T12:28:52.072Z","end":"2023-12-12T12:28:52.183Z","steps":["trace[1111605039] 'process raft request'  (duration: 110.943482ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:00.368Z","caller":"traceutil/trace.go:171","msg":"trace[862330448] transaction","detail":"{read_only:false; response_revision:118615; number_of_response:1; }","duration":"112.573916ms","start":"2023-12-12T12:29:00.255Z","end":"2023-12-12T12:29:00.368Z","steps":["trace[862330448] 'process raft request'  (duration: 112.386165ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:06.531Z","caller":"traceutil/trace.go:171","msg":"trace[1277403406] transaction","detail":"{read_only:false; response_revision:118621; number_of_response:1; }","duration":"111.047324ms","start":"2023-12-12T12:29:06.420Z","end":"2023-12-12T12:29:06.531Z","steps":["trace[1277403406] 'process raft request'  (duration: 110.710895ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:08.655Z","caller":"traceutil/trace.go:171","msg":"trace[1645280888] transaction","detail":"{read_only:false; response_revision:118623; number_of_response:1; }","duration":"111.427029ms","start":"2023-12-12T12:29:08.544Z","end":"2023-12-12T12:29:08.655Z","steps":["trace[1645280888] 'process raft request'  (duration: 111.222244ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:10.780Z","caller":"traceutil/trace.go:171","msg":"trace[1349449182] transaction","detail":"{read_only:false; response_revision:118624; number_of_response:1; }","duration":"112.991653ms","start":"2023-12-12T12:29:10.667Z","end":"2023-12-12T12:29:10.780Z","steps":["trace[1349449182] 'process raft request'  (duration: 112.751906ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:14.922Z","caller":"traceutil/trace.go:171","msg":"trace[397704709] transaction","detail":"{read_only:false; response_revision:118629; number_of_response:1; }","duration":"111.645207ms","start":"2023-12-12T12:29:14.811Z","end":"2023-12-12T12:29:14.922Z","steps":["trace[397704709] 'process raft request'  (duration: 111.424335ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:21.078Z","caller":"traceutil/trace.go:171","msg":"trace[637146059] transaction","detail":"{read_only:false; response_revision:118632; number_of_response:1; }","duration":"111.136488ms","start":"2023-12-12T12:29:20.967Z","end":"2023-12-12T12:29:21.078Z","steps":["trace[637146059] 'process raft request'  (duration: 110.772848ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:27.240Z","caller":"traceutil/trace.go:171","msg":"trace[608755020] transaction","detail":"{read_only:false; response_revision:118638; number_of_response:1; }","duration":"111.171966ms","start":"2023-12-12T12:29:27.129Z","end":"2023-12-12T12:29:27.240Z","steps":["trace[608755020] 'process raft request'  (duration: 110.949264ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:31.383Z","caller":"traceutil/trace.go:171","msg":"trace[799862892] transaction","detail":"{read_only:false; response_revision:118640; number_of_response:1; }","duration":"110.875796ms","start":"2023-12-12T12:29:31.272Z","end":"2023-12-12T12:29:31.383Z","steps":["trace[799862892] 'process raft request'  (duration: 110.628702ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:33.512Z","caller":"traceutil/trace.go:171","msg":"trace[1436671973] transaction","detail":"{read_only:false; response_revision:118642; number_of_response:1; }","duration":"112.781642ms","start":"2023-12-12T12:29:33.399Z","end":"2023-12-12T12:29:33.512Z","steps":["trace[1436671973] 'process raft request'  (duration: 112.596823ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:35.636Z","caller":"traceutil/trace.go:171","msg":"trace[839430954] linearizableReadLoop","detail":"{readStateIndex:147887; appliedIndex:147886; }","duration":"107.548331ms","start":"2023-12-12T12:29:35.529Z","end":"2023-12-12T12:29:35.636Z","steps":["trace[839430954] 'read index received'  (duration: 107.252214ms)","trace[839430954] 'applied index is now lower than readState.Index'  (duration: 294.497¬µs)"],"step_count":2}
{"level":"info","ts":"2023-12-12T12:29:35.636Z","caller":"traceutil/trace.go:171","msg":"trace[1214734090] transaction","detail":"{read_only:false; response_revision:118645; number_of_response:1; }","duration":"111.81861ms","start":"2023-12-12T12:29:35.524Z","end":"2023-12-12T12:29:35.636Z","steps":["trace[1214734090] 'process raft request'  (duration: 111.508308ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-12T12:29:35.636Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"107.838429ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-12T12:29:35.636Z","caller":"traceutil/trace.go:171","msg":"trace[2094373713] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:118645; }","duration":"107.982563ms","start":"2023-12-12T12:29:35.528Z","end":"2023-12-12T12:29:35.636Z","steps":["trace[2094373713] 'agreement among raft nodes before linearized reading'  (duration: 107.776293ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:41.803Z","caller":"traceutil/trace.go:171","msg":"trace[1487518768] transaction","detail":"{read_only:false; response_revision:118648; number_of_response:1; }","duration":"111.064444ms","start":"2023-12-12T12:29:41.692Z","end":"2023-12-12T12:29:41.803Z","steps":["trace[1487518768] 'process raft request'  (duration: 110.87361ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:45.952Z","caller":"traceutil/trace.go:171","msg":"trace[805609139] transaction","detail":"{read_only:false; response_revision:118653; number_of_response:1; }","duration":"114.916124ms","start":"2023-12-12T12:29:45.837Z","end":"2023-12-12T12:29:45.952Z","steps":["trace[805609139] 'process raft request'  (duration: 114.718452ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:50.095Z","caller":"traceutil/trace.go:171","msg":"trace[1355670551] transaction","detail":"{read_only:false; response_revision:118655; number_of_response:1; }","duration":"110.993249ms","start":"2023-12-12T12:29:49.984Z","end":"2023-12-12T12:29:50.095Z","steps":["trace[1355670551] 'process raft request'  (duration: 110.789143ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:52.203Z","caller":"traceutil/trace.go:171","msg":"trace[77892459] linearizableReadLoop","detail":"{readStateIndex:147901; appliedIndex:147900; }","duration":"104.24776ms","start":"2023-12-12T12:29:52.098Z","end":"2023-12-12T12:29:52.203Z","steps":["trace[77892459] 'read index received'  (duration: 104.04949ms)","trace[77892459] 'applied index is now lower than readState.Index'  (duration: 196.029¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-12-12T12:29:52.203Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"104.430618ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2023-12-12T12:29:52.203Z","caller":"traceutil/trace.go:171","msg":"trace[683735780] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:118655; }","duration":"104.526545ms","start":"2023-12-12T12:29:52.098Z","end":"2023-12-12T12:29:52.203Z","steps":["trace[683735780] 'agreement among raft nodes before linearized reading'  (duration: 104.376932ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:29:56.358Z","caller":"traceutil/trace.go:171","msg":"trace[1278656154] transaction","detail":"{read_only:false; response_revision:118661; number_of_response:1; }","duration":"112.530104ms","start":"2023-12-12T12:29:56.246Z","end":"2023-12-12T12:29:56.358Z","steps":["trace[1278656154] 'process raft request'  (duration: 112.432385ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:30:00.498Z","caller":"traceutil/trace.go:171","msg":"trace[432775752] linearizableReadLoop","detail":"{readStateIndex:147910; appliedIndex:147909; }","duration":"107.830684ms","start":"2023-12-12T12:30:00.390Z","end":"2023-12-12T12:30:00.498Z","steps":["trace[432775752] 'read index received'  (duration: 107.530998ms)","trace[432775752] 'applied index is now lower than readState.Index'  (duration: 297.908¬µs)"],"step_count":2}
{"level":"info","ts":"2023-12-12T12:30:00.498Z","caller":"traceutil/trace.go:171","msg":"trace[963156274] transaction","detail":"{read_only:false; response_revision:118663; number_of_response:1; }","duration":"110.98993ms","start":"2023-12-12T12:30:00.387Z","end":"2023-12-12T12:30:00.498Z","steps":["trace[963156274] 'process raft request'  (duration: 110.761713ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-12T12:30:00.498Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"108.067957ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-12T12:30:00.498Z","caller":"traceutil/trace.go:171","msg":"trace[178173339] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:118663; }","duration":"108.170355ms","start":"2023-12-12T12:30:00.390Z","end":"2023-12-12T12:30:00.498Z","steps":["trace[178173339] 'agreement among raft nodes before linearized reading'  (duration: 108.022618ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:30:06.660Z","caller":"traceutil/trace.go:171","msg":"trace[466430642] transaction","detail":"{read_only:false; response_revision:118669; number_of_response:1; }","duration":"111.372137ms","start":"2023-12-12T12:30:06.548Z","end":"2023-12-12T12:30:06.660Z","steps":["trace[466430642] 'process raft request'  (duration: 111.148124ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:30:08.782Z","caller":"traceutil/trace.go:171","msg":"trace[1598813298] transaction","detail":"{read_only:false; response_revision:118670; number_of_response:1; }","duration":"110.706063ms","start":"2023-12-12T12:30:08.671Z","end":"2023-12-12T12:30:08.782Z","steps":["trace[1598813298] 'process raft request'  (duration: 110.556663ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:30:18.984Z","caller":"traceutil/trace.go:171","msg":"trace[1197959778] transaction","detail":"{read_only:false; response_revision:118678; number_of_response:1; }","duration":"116.154249ms","start":"2023-12-12T12:30:18.868Z","end":"2023-12-12T12:30:18.984Z","steps":["trace[1197959778] 'process raft request'  (duration: 115.940857ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:30:27.172Z","caller":"traceutil/trace.go:171","msg":"trace[2012589129] transaction","detail":"{read_only:false; response_revision:118685; number_of_response:1; }","duration":"110.883807ms","start":"2023-12-12T12:30:27.061Z","end":"2023-12-12T12:30:27.172Z","steps":["trace[2012589129] 'process raft request'  (duration: 110.689669ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:30:29.297Z","caller":"traceutil/trace.go:171","msg":"trace[1199367920] transaction","detail":"{read_only:false; response_revision:118686; number_of_response:1; }","duration":"112.782032ms","start":"2023-12-12T12:30:29.184Z","end":"2023-12-12T12:30:29.297Z","steps":["trace[1199367920] 'process raft request'  (duration: 112.547354ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:30:31.423Z","caller":"traceutil/trace.go:171","msg":"trace[1246759880] transaction","detail":"{read_only:false; response_revision:118687; number_of_response:1; }","duration":"112.377312ms","start":"2023-12-12T12:30:31.310Z","end":"2023-12-12T12:30:31.423Z","steps":["trace[1246759880] 'process raft request'  (duration: 111.955673ms)"],"step_count":1}
{"level":"info","ts":"2023-12-12T12:30:33.548Z","caller":"traceutil/trace.go:171","msg":"trace[120760965] transaction","detail":"{read_only:false; response_revision:118689; number_of_response:1; }","duration":"111.510194ms","start":"2023-12-12T12:30:33.437Z","end":"2023-12-12T12:30:33.548Z","steps":["trace[120760965] 'process raft request'  (duration: 111.00754ms)"],"step_count":1}

* 
* ==> etcd [ef3dc081cbb2] <==
* {"level":"info","ts":"2023-12-11T10:21:56.954Z","caller":"traceutil/trace.go:171","msg":"trace[685906357] transaction","detail":"{read_only:false; response_revision:117362; number_of_response:1; }","duration":"111.10024ms","start":"2023-12-11T10:21:56.843Z","end":"2023-12-11T10:21:56.954Z","steps":["trace[685906357] 'process raft request'  (duration: 110.824323ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:01.563Z","caller":"traceutil/trace.go:171","msg":"trace[1989570133] transaction","detail":"{read_only:false; response_revision:117366; number_of_response:1; }","duration":"114.165383ms","start":"2023-12-11T10:22:01.449Z","end":"2023-12-11T10:22:01.563Z","steps":["trace[1989570133] 'process raft request'  (duration: 113.43376ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:03.687Z","caller":"traceutil/trace.go:171","msg":"trace[685102671] transaction","detail":"{read_only:false; response_revision:117367; number_of_response:1; }","duration":"111.2927ms","start":"2023-12-11T10:22:03.576Z","end":"2023-12-11T10:22:03.687Z","steps":["trace[685102671] 'process raft request'  (duration: 111.069425ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:07.284Z","caller":"traceutil/trace.go:171","msg":"trace[825602191] transaction","detail":"{read_only:false; response_revision:117370; number_of_response:1; }","duration":"113.05713ms","start":"2023-12-11T10:22:07.171Z","end":"2023-12-11T10:22:07.284Z","steps":["trace[825602191] 'process raft request'  (duration: 112.785192ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:11.878Z","caller":"traceutil/trace.go:171","msg":"trace[1958967294] transaction","detail":"{read_only:false; response_revision:117374; number_of_response:1; }","duration":"111.16998ms","start":"2023-12-11T10:22:11.767Z","end":"2023-12-11T10:22:11.878Z","steps":["trace[1958967294] 'process raft request'  (duration: 110.835554ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:17.703Z","caller":"traceutil/trace.go:171","msg":"trace[1542719461] transaction","detail":"{read_only:false; response_revision:117378; number_of_response:1; }","duration":"111.207777ms","start":"2023-12-11T10:22:17.592Z","end":"2023-12-11T10:22:17.703Z","steps":["trace[1542719461] 'process raft request'  (duration: 110.999292ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:22.090Z","caller":"traceutil/trace.go:171","msg":"trace[1047530435] transaction","detail":"{read_only:false; response_revision:117382; number_of_response:1; }","duration":"111.152576ms","start":"2023-12-11T10:22:21.979Z","end":"2023-12-11T10:22:22.090Z","steps":["trace[1047530435] 'process raft request'  (duration: 110.757303ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:26.241Z","caller":"traceutil/trace.go:171","msg":"trace[437549523] transaction","detail":"{read_only:false; response_revision:117385; number_of_response:1; }","duration":"110.98761ms","start":"2023-12-11T10:22:26.130Z","end":"2023-12-11T10:22:26.241Z","steps":["trace[437549523] 'process raft request'  (duration: 110.717724ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:28.007Z","caller":"traceutil/trace.go:171","msg":"trace[1662061611] transaction","detail":"{read_only:false; response_revision:117386; number_of_response:1; }","duration":"111.126174ms","start":"2023-12-11T10:22:27.896Z","end":"2023-12-11T10:22:28.007Z","steps":["trace[1662061611] 'process raft request'  (duration: 110.837133ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:30.391Z","caller":"traceutil/trace.go:171","msg":"trace[489366996] transaction","detail":"{read_only:false; response_revision:117389; number_of_response:1; }","duration":"111.01448ms","start":"2023-12-11T10:22:30.280Z","end":"2023-12-11T10:22:30.391Z","steps":["trace[489366996] 'process raft request'  (duration: 110.706816ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:42.641Z","caller":"traceutil/trace.go:171","msg":"trace[2108246573] transaction","detail":"{read_only:false; response_revision:117398; number_of_response:1; }","duration":"111.142632ms","start":"2023-12-11T10:22:42.530Z","end":"2023-12-11T10:22:42.641Z","steps":["trace[2108246573] 'process raft request'  (duration: 110.883068ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:22:48.706Z","caller":"traceutil/trace.go:171","msg":"trace[1397606941] transaction","detail":"{read_only:false; response_revision:117402; number_of_response:1; }","duration":"111.184487ms","start":"2023-12-11T10:22:48.595Z","end":"2023-12-11T10:22:48.706Z","steps":["trace[1397606941] 'process raft request'  (duration: 110.745071ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:00.958Z","caller":"traceutil/trace.go:171","msg":"trace[1248225139] transaction","detail":"{read_only:false; response_revision:117414; number_of_response:1; }","duration":"111.217576ms","start":"2023-12-11T10:23:00.846Z","end":"2023-12-11T10:23:00.957Z","steps":["trace[1248225139] 'process raft request'  (duration: 110.902454ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:03.083Z","caller":"traceutil/trace.go:171","msg":"trace[461752278] transaction","detail":"{read_only:false; response_revision:117415; number_of_response:1; }","duration":"112.40298ms","start":"2023-12-11T10:23:02.971Z","end":"2023-12-11T10:23:03.083Z","steps":["trace[461752278] 'process raft request'  (duration: 112.169547ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:05.207Z","caller":"traceutil/trace.go:171","msg":"trace[1086528093] transaction","detail":"{read_only:false; response_revision:117417; number_of_response:1; }","duration":"110.928636ms","start":"2023-12-11T10:23:05.096Z","end":"2023-12-11T10:23:05.207Z","steps":["trace[1086528093] 'process raft request'  (duration: 110.676596ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:06.930Z","caller":"traceutil/trace.go:171","msg":"trace[982375131] transaction","detail":"{read_only:false; response_revision:117418; number_of_response:1; }","duration":"112.810048ms","start":"2023-12-11T10:23:06.817Z","end":"2023-12-11T10:23:06.930Z","steps":["trace[982375131] 'process raft request'  (duration: 112.603395ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:08.964Z","caller":"traceutil/trace.go:171","msg":"trace[558334288] transaction","detail":"{read_only:false; response_revision:117420; number_of_response:1; }","duration":"112.931273ms","start":"2023-12-11T10:23:08.851Z","end":"2023-12-11T10:23:08.964Z","steps":["trace[558334288] 'process raft request'  (duration: 112.526794ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:13.392Z","caller":"traceutil/trace.go:171","msg":"trace[1635927201] transaction","detail":"{read_only:false; response_revision:117424; number_of_response:1; }","duration":"112.712679ms","start":"2023-12-11T10:23:13.279Z","end":"2023-12-11T10:23:13.392Z","steps":["trace[1635927201] 'process raft request'  (duration: 112.504351ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:21.584Z","caller":"traceutil/trace.go:171","msg":"trace[40200084] transaction","detail":"{read_only:false; response_revision:117431; number_of_response:1; }","duration":"112.668797ms","start":"2023-12-11T10:23:21.471Z","end":"2023-12-11T10:23:21.584Z","steps":["trace[40200084] 'process raft request'  (duration: 112.379273ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:27.736Z","caller":"traceutil/trace.go:171","msg":"trace[1459665187] transaction","detail":"{read_only:false; response_revision:117435; number_of_response:1; }","duration":"113.640297ms","start":"2023-12-11T10:23:27.622Z","end":"2023-12-11T10:23:27.736Z","steps":["trace[1459665187] 'process raft request'  (duration: 113.454127ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:31.886Z","caller":"traceutil/trace.go:171","msg":"trace[1923058414] transaction","detail":"{read_only:false; response_revision:117439; number_of_response:1; }","duration":"110.940674ms","start":"2023-12-11T10:23:31.775Z","end":"2023-12-11T10:23:31.886Z","steps":["trace[1923058414] 'process raft request'  (duration: 110.734941ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:34.008Z","caller":"traceutil/trace.go:171","msg":"trace[1665820299] transaction","detail":"{read_only:false; response_revision:117440; number_of_response:1; }","duration":"111.14093ms","start":"2023-12-11T10:23:33.897Z","end":"2023-12-11T10:23:34.008Z","steps":["trace[1665820299] 'process raft request'  (duration: 110.806018ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:38.157Z","caller":"traceutil/trace.go:171","msg":"trace[1477834428] transaction","detail":"{read_only:false; response_revision:117443; number_of_response:1; }","duration":"110.976224ms","start":"2023-12-11T10:23:38.046Z","end":"2023-12-11T10:23:38.157Z","steps":["trace[1477834428] 'process raft request'  (duration: 110.750471ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:48.374Z","caller":"traceutil/trace.go:171","msg":"trace[1270968269] transaction","detail":"{read_only:false; response_revision:117451; number_of_response:1; }","duration":"111.348964ms","start":"2023-12-11T10:23:48.263Z","end":"2023-12-11T10:23:48.374Z","steps":["trace[1270968269] 'process raft request'  (duration: 111.145713ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:23:52.519Z","caller":"traceutil/trace.go:171","msg":"trace[2138119936] transaction","detail":"{read_only:false; response_revision:117455; number_of_response:1; }","duration":"111.061487ms","start":"2023-12-11T10:23:52.408Z","end":"2023-12-11T10:23:52.519Z","steps":["trace[2138119936] 'process raft request'  (duration: 110.800619ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:24:02.732Z","caller":"traceutil/trace.go:171","msg":"trace[665925284] transaction","detail":"{read_only:false; response_revision:117463; number_of_response:1; }","duration":"111.065806ms","start":"2023-12-11T10:24:02.621Z","end":"2023-12-11T10:24:02.732Z","steps":["trace[665925284] 'process raft request'  (duration: 110.852189ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:24:10.463Z","caller":"traceutil/trace.go:171","msg":"trace[2039935487] transaction","detail":"{read_only:false; response_revision:117468; number_of_response:1; }","duration":"110.951303ms","start":"2023-12-11T10:24:10.352Z","end":"2023-12-11T10:24:10.463Z","steps":["trace[2039935487] 'process raft request'  (duration: 110.708694ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:24:12.944Z","caller":"traceutil/trace.go:171","msg":"trace[1944926341] transaction","detail":"{read_only:false; response_revision:117471; number_of_response:1; }","duration":"113.218496ms","start":"2023-12-11T10:24:12.831Z","end":"2023-12-11T10:24:12.944Z","steps":["trace[1944926341] 'process raft request'  (duration: 112.80941ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:24:29.228Z","caller":"traceutil/trace.go:171","msg":"trace[961736780] transaction","detail":"{read_only:false; response_revision:117483; number_of_response:1; }","duration":"111.06767ms","start":"2023-12-11T10:24:29.117Z","end":"2023-12-11T10:24:29.228Z","steps":["trace[961736780] 'process raft request'  (duration: 110.860346ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:24:31.156Z","caller":"traceutil/trace.go:171","msg":"trace[2076784584] transaction","detail":"{read_only:false; response_revision:117484; number_of_response:1; }","duration":"111.098901ms","start":"2023-12-11T10:24:31.045Z","end":"2023-12-11T10:24:31.156Z","steps":["trace[2076784584] 'process raft request'  (duration: 110.782398ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:24:35.391Z","caller":"traceutil/trace.go:171","msg":"trace[1444010520] transaction","detail":"{read_only:false; response_revision:117489; number_of_response:1; }","duration":"113.413722ms","start":"2023-12-11T10:24:35.278Z","end":"2023-12-11T10:24:35.391Z","steps":["trace[1444010520] 'process raft request'  (duration: 113.21127ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:24:53.682Z","caller":"traceutil/trace.go:171","msg":"trace[366625624] transaction","detail":"{read_only:false; response_revision:117503; number_of_response:1; }","duration":"112.811718ms","start":"2023-12-11T10:24:53.569Z","end":"2023-12-11T10:24:53.682Z","steps":["trace[366625624] 'process raft request'  (duration: 112.619999ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:24:55.805Z","caller":"traceutil/trace.go:171","msg":"trace[181854635] transaction","detail":"{read_only:false; response_revision:117505; number_of_response:1; }","duration":"110.869086ms","start":"2023-12-11T10:24:55.694Z","end":"2023-12-11T10:24:55.805Z","steps":["trace[181854635] 'process raft request'  (duration: 110.669759ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:06.020Z","caller":"traceutil/trace.go:171","msg":"trace[1499794733] transaction","detail":"{read_only:false; response_revision:117513; number_of_response:1; }","duration":"111.025046ms","start":"2023-12-11T10:25:05.908Z","end":"2023-12-11T10:25:06.020Z","steps":["trace[1499794733] 'process raft request'  (duration: 110.778712ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:08.143Z","caller":"traceutil/trace.go:171","msg":"trace[2127015644] transaction","detail":"{read_only:false; response_revision:117514; number_of_response:1; }","duration":"110.994843ms","start":"2023-12-11T10:25:08.032Z","end":"2023-12-11T10:25:08.143Z","steps":["trace[2127015644] 'process raft request'  (duration: 110.778435ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:12.008Z","caller":"traceutil/trace.go:171","msg":"trace[254635091] transaction","detail":"{read_only:false; response_revision:117516; number_of_response:1; }","duration":"111.714544ms","start":"2023-12-11T10:25:11.896Z","end":"2023-12-11T10:25:12.008Z","steps":["trace[254635091] 'process raft request'  (duration: 111.437469ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:22.144Z","caller":"traceutil/trace.go:171","msg":"trace[243533270] transaction","detail":"{read_only:false; response_revision:117524; number_of_response:1; }","duration":"111.202772ms","start":"2023-12-11T10:25:22.033Z","end":"2023-12-11T10:25:22.144Z","steps":["trace[243533270] 'process raft request'  (duration: 110.847407ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:26.454Z","caller":"traceutil/trace.go:171","msg":"trace[1823099273] transaction","detail":"{read_only:false; response_revision:117529; number_of_response:1; }","duration":"113.344746ms","start":"2023-12-11T10:25:26.340Z","end":"2023-12-11T10:25:26.454Z","steps":["trace[1823099273] 'process raft request'  (duration: 113.103888ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:28.577Z","caller":"traceutil/trace.go:171","msg":"trace[2089107017] transaction","detail":"{read_only:false; response_revision:117530; number_of_response:1; }","duration":"111.071902ms","start":"2023-12-11T10:25:28.466Z","end":"2023-12-11T10:25:28.577Z","steps":["trace[2089107017] 'process raft request'  (duration: 110.844537ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:36.763Z","caller":"traceutil/trace.go:171","msg":"trace[946175330] transaction","detail":"{read_only:false; response_revision:117537; number_of_response:1; }","duration":"113.040998ms","start":"2023-12-11T10:25:36.650Z","end":"2023-12-11T10:25:36.763Z","steps":["trace[946175330] 'process raft request'  (duration: 112.736725ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:38.889Z","caller":"traceutil/trace.go:171","msg":"trace[1933127483] transaction","detail":"{read_only:false; response_revision:117538; number_of_response:1; }","duration":"111.018401ms","start":"2023-12-11T10:25:38.778Z","end":"2023-12-11T10:25:38.889Z","steps":["trace[1933127483] 'process raft request'  (duration: 110.824055ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:25:47.237Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":117306}
{"level":"info","ts":"2023-12-11T10:25:47.241Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":117306,"took":"2.898524ms","hash":2161249945}
{"level":"info","ts":"2023-12-11T10:25:47.242Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2161249945,"revision":117306,"compact-revision":117065}
{"level":"info","ts":"2023-12-11T10:25:59.245Z","caller":"traceutil/trace.go:171","msg":"trace[494754210] transaction","detail":"{read_only:false; response_revision:117558; number_of_response:1; }","duration":"121.486813ms","start":"2023-12-11T10:25:59.123Z","end":"2023-12-11T10:25:59.245Z","steps":["trace[494754210] 'process raft request'  (duration: 121.298693ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:26:05.414Z","caller":"traceutil/trace.go:171","msg":"trace[510820367] transaction","detail":"{read_only:false; response_revision:117564; number_of_response:1; }","duration":"111.014889ms","start":"2023-12-11T10:26:05.303Z","end":"2023-12-11T10:26:05.414Z","steps":["trace[510820367] 'process raft request'  (duration: 110.752101ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:26:09.559Z","caller":"traceutil/trace.go:171","msg":"trace[1238198831] transaction","detail":"{read_only:false; response_revision:117566; number_of_response:1; }","duration":"110.970958ms","start":"2023-12-11T10:26:09.448Z","end":"2023-12-11T10:26:09.559Z","steps":["trace[1238198831] 'process raft request'  (duration: 110.645389ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:26:35.184Z","caller":"traceutil/trace.go:171","msg":"trace[915426099] transaction","detail":"{read_only:false; response_revision:117589; number_of_response:1; }","duration":"113.330709ms","start":"2023-12-11T10:26:35.070Z","end":"2023-12-11T10:26:35.183Z","steps":["trace[915426099] 'process raft request'  (duration: 113.121917ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:26:56.169Z","caller":"traceutil/trace.go:171","msg":"trace[1535763399] transaction","detail":"{read_only:false; response_revision:117606; number_of_response:1; }","duration":"112.788165ms","start":"2023-12-11T10:26:56.057Z","end":"2023-12-11T10:26:56.169Z","steps":["trace[1535763399] 'process raft request'  (duration: 112.571108ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:27:00.315Z","caller":"traceutil/trace.go:171","msg":"trace[1522158991] transaction","detail":"{read_only:false; response_revision:117608; number_of_response:1; }","duration":"111.063954ms","start":"2023-12-11T10:27:00.204Z","end":"2023-12-11T10:27:00.315Z","steps":["trace[1522158991] 'process raft request'  (duration: 110.797845ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:27:22.653Z","caller":"traceutil/trace.go:171","msg":"trace[1345050472] transaction","detail":"{read_only:false; response_revision:117625; number_of_response:1; }","duration":"111.043014ms","start":"2023-12-11T10:27:22.542Z","end":"2023-12-11T10:27:22.653Z","steps":["trace[1345050472] 'process raft request'  (duration: 110.741239ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:27:26.805Z","caller":"traceutil/trace.go:171","msg":"trace[444950075] transaction","detail":"{read_only:false; response_revision:117630; number_of_response:1; }","duration":"111.066256ms","start":"2023-12-11T10:27:26.694Z","end":"2023-12-11T10:27:26.805Z","steps":["trace[444950075] 'process raft request'  (duration: 110.783259ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:27:28.933Z","caller":"traceutil/trace.go:171","msg":"trace[460837012] transaction","detail":"{read_only:false; response_revision:117631; number_of_response:1; }","duration":"113.61805ms","start":"2023-12-11T10:27:28.820Z","end":"2023-12-11T10:27:28.933Z","steps":["trace[460837012] 'process raft request'  (duration: 113.220908ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:27:31.058Z","caller":"traceutil/trace.go:171","msg":"trace[178215054] transaction","detail":"{read_only:false; response_revision:117632; number_of_response:1; }","duration":"111.19379ms","start":"2023-12-11T10:27:30.946Z","end":"2023-12-11T10:27:31.058Z","steps":["trace[178215054] 'process raft request'  (duration: 110.943437ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T10:27:31.763Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-12-11T10:27:31.763Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-12-11T10:27:31.875Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-12-11T10:27:31.878Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-11T10:27:31.880Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-11T10:27:31.880Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  12:30:37 up  2:19,  0 users,  load average: 0.70, 1.26, 1.28
Linux minikube 6.2.0-37-generic #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [8882b6c8ffa3] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1211 10:27:41.425193       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1211 10:27:41.444103       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1211 10:27:41.497545       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1211 10:27:41.497565       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1211 10:27:41.523581       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1211 10:27:41.529719       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1211 10:27:41.593900       1 logging.go:59] [core] [Channel #66 SubChannel #67] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [8f339a025a83] <==
* I1212 12:11:50.682094       1 crd_finalizer.go:266] Starting CRDFinalizer
I1212 12:11:50.682121       1 shared_informer.go:273] Waiting for caches to sync for crd-autoregister
I1212 12:11:50.682107       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1212 12:11:50.682302       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1212 12:11:50.689290       1 controller.go:121] Starting legacy_token_tracking_controller
I1212 12:11:50.689299       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1212 12:11:50.689302       1 shared_informer.go:273] Waiting for caches to sync for configmaps
I1212 12:11:50.690120       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1212 12:11:50.696096       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1212 12:11:50.696113       1 shared_informer.go:273] Waiting for caches to sync for cluster_authentication_trust_controller
I1212 12:11:50.697035       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1212 12:11:50.697112       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1212 12:11:50.713827       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E1212 12:11:50.715185       1 controller.go:159] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I1212 12:11:50.763044       1 shared_informer.go:280] Caches are synced for node_authorizer
I1212 12:11:50.782029       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1212 12:11:50.782043       1 cache.go:39] Caches are synced for autoregister controller
I1212 12:11:50.782214       1 apf_controller.go:366] Running API Priority and Fairness config worker
I1212 12:11:50.782234       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I1212 12:11:50.782292       1 shared_informer.go:280] Caches are synced for crd-autoregister
I1212 12:11:50.782316       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1212 12:11:50.789557       1 shared_informer.go:280] Caches are synced for configmaps
I1212 12:11:50.797215       1 shared_informer.go:280] Caches are synced for cluster_authentication_trust_controller
I1212 12:11:51.524941       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1212 12:11:51.691095       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1212 12:11:52.471244       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1212 12:11:52.478740       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1212 12:11:52.515629       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1212 12:11:52.535423       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1212 12:11:52.540506       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
E1212 12:11:55.003814       1 storage.go:469] Address {10.244.0.174  0xc00c72cc80 0xc000b319d0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 10.244.0.174 (kubernetes-dashboard/dashboard-metrics-scraper-5c6664855-hxh7v))
E1212 12:11:55.003841       1 storage.go:479] Failed to find a valid address, skipping subset: &{[{10.244.0.174  0xc00c72cc80 0xc000b319d0}] [] [{ 8000 TCP <nil>}]}
E1212 12:11:55.794504       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
E1212 12:11:58.883703       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: no route to host
E1212 12:12:01.949621       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: no route to host
E1212 12:12:05.017818       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: no route to host
E1212 12:12:08.089480       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: no route to host
I1212 12:12:08.313894       1 controller.go:615] quota admission added evaluator for: endpoints
I1212 12:12:08.332859       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E1212 12:12:11.161710       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: no route to host
E1212 12:12:21.803714       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1212 12:12:21.803752       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1212 12:12:21.803748       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: error trying to reach service: dial tcp 10.101.123.84:443: i/o timeout
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1212 12:12:21.805092       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1212 12:12:22.804940       1 handler_proxy.go:106] no RequestInfo found in the context
E1212 12:12:22.805006       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1212 12:12:22.805026       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1212 12:12:57.903969       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: connection refused
E1212 12:12:57.904543       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: connection refused
E1212 12:12:57.909763       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: connection refused
E1212 12:12:57.930875       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.101.123.84:443: connect: connection refused
W1212 12:12:58.905421       1 handler_proxy.go:106] no RequestInfo found in the context
W1212 12:12:58.905464       1 handler_proxy.go:106] no RequestInfo found in the context
E1212 12:12:58.905499       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1212 12:12:58.905521       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1212 12:12:58.905652       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1212 12:12:58.906946       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1212 12:13:02.991244       1 available_controller.go:527] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1: Get "https://10.101.123.84:443/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)

* 
* ==> kube-controller-manager [adb9a38bb9fb] <==
* I1212 12:12:08.110253       1 shared_informer.go:273] Waiting for caches to sync for resource quota
W1212 12:12:08.111995       1 topologycache.go:232] Can't get CPU or zone information for minikube node
W1212 12:12:08.112034       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I1212 12:12:08.115643       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1212 12:12:08.115685       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
E1212 12:12:08.123608       1 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E1212 12:12:08.125324       1 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I1212 12:12:08.126994       1 shared_informer.go:273] Waiting for caches to sync for garbage collector
I1212 12:12:08.154865       1 shared_informer.go:280] Caches are synced for crt configmap
I1212 12:12:08.164511       1 shared_informer.go:280] Caches are synced for cronjob
I1212 12:12:08.166711       1 shared_informer.go:280] Caches are synced for bootstrap_signer
I1212 12:12:08.173458       1 shared_informer.go:280] Caches are synced for node
I1212 12:12:08.173546       1 range_allocator.go:167] Sending events to api server.
I1212 12:12:08.173629       1 range_allocator.go:171] Starting range CIDR allocator
I1212 12:12:08.173648       1 shared_informer.go:273] Waiting for caches to sync for cidrallocator
I1212 12:12:08.173668       1 shared_informer.go:280] Caches are synced for cidrallocator
I1212 12:12:08.175818       1 shared_informer.go:280] Caches are synced for ClusterRoleAggregator
I1212 12:12:08.180040       1 shared_informer.go:280] Caches are synced for expand
I1212 12:12:08.188093       1 shared_informer.go:280] Caches are synced for certificate-csrapproving
I1212 12:12:08.194621       1 shared_informer.go:280] Caches are synced for PV protection
I1212 12:12:08.196524       1 shared_informer.go:280] Caches are synced for service account
I1212 12:12:08.200247       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-serving
I1212 12:12:08.200564       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-client
I1212 12:12:08.201601       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-legacy-unknown
I1212 12:12:08.201656       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1212 12:12:08.205335       1 shared_informer.go:280] Caches are synced for TTL after finished
I1212 12:12:08.205416       1 shared_informer.go:280] Caches are synced for namespace
I1212 12:12:08.208168       1 shared_informer.go:280] Caches are synced for TTL
I1212 12:12:08.267150       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1212 12:12:08.267531       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1212 12:12:08.270795       1 shared_informer.go:280] Caches are synced for endpoint_slice_mirroring
I1212 12:12:08.276221       1 shared_informer.go:280] Caches are synced for ReplicaSet
I1212 12:12:08.278910       1 shared_informer.go:280] Caches are synced for persistent volume
I1212 12:12:08.282235       1 shared_informer.go:280] Caches are synced for endpoint
I1212 12:12:08.292109       1 shared_informer.go:280] Caches are synced for taint
I1212 12:12:08.292426       1 node_lifecycle_controller.go:1438] Initializing eviction metric for zone: 
I1212 12:12:08.292451       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
W1212 12:12:08.292640       1 node_lifecycle_controller.go:1053] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1212 12:12:08.293055       1 node_lifecycle_controller.go:1254] Controller detected that zone  is now in state Normal.
I1212 12:12:08.292663       1 taint_manager.go:211] "Sending events to api server"
I1212 12:12:08.293283       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1212 12:12:08.297898       1 shared_informer.go:280] Caches are synced for endpoint_slice
I1212 12:12:08.299231       1 shared_informer.go:280] Caches are synced for daemon sets
I1212 12:12:08.300993       1 shared_informer.go:280] Caches are synced for job
I1212 12:12:08.304072       1 shared_informer.go:280] Caches are synced for disruption
I1212 12:12:08.305987       1 shared_informer.go:280] Caches are synced for ephemeral
I1212 12:12:08.306970       1 shared_informer.go:280] Caches are synced for stateful set
I1212 12:12:08.349149       1 shared_informer.go:280] Caches are synced for deployment
I1212 12:12:08.351871       1 shared_informer.go:280] Caches are synced for attach detach
I1212 12:12:08.353520       1 shared_informer.go:280] Caches are synced for PVC protection
I1212 12:12:08.353550       1 shared_informer.go:280] Caches are synced for ReplicationController
I1212 12:12:08.357973       1 shared_informer.go:280] Caches are synced for GC
I1212 12:12:08.386617       1 shared_informer.go:280] Caches are synced for HPA
I1212 12:12:08.394303       1 shared_informer.go:280] Caches are synced for resource quota
I1212 12:12:08.410544       1 shared_informer.go:280] Caches are synced for resource quota
I1212 12:12:08.727688       1 shared_informer.go:280] Caches are synced for garbage collector
I1212 12:12:08.762159       1 shared_informer.go:280] Caches are synced for garbage collector
I1212 12:12:08.762206       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
E1212 12:12:38.428441       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1212 12:12:38.759359       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]

* 
* ==> kube-controller-manager [e5305624239e] <==
* I1211 09:56:08.935590       1 shared_informer.go:273] Waiting for caches to sync for PV protection
I1211 09:56:08.954295       1 shared_informer.go:273] Waiting for caches to sync for resource quota
W1211 09:56:08.966432       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
W1211 09:56:08.980617       1 topologycache.go:232] Can't get CPU or zone information for minikube node
W1211 09:56:08.980785       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I1211 09:56:08.984657       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1211 09:56:08.984758       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1211 09:56:08.987333       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1211 09:56:08.987674       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
E1211 09:56:08.993451       1 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I1211 09:56:08.999173       1 shared_informer.go:280] Caches are synced for namespace
E1211 09:56:08.999252       1 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I1211 09:56:09.003049       1 shared_informer.go:280] Caches are synced for expand
I1211 09:56:09.008489       1 shared_informer.go:280] Caches are synced for ephemeral
I1211 09:56:09.009747       1 shared_informer.go:273] Waiting for caches to sync for garbage collector
I1211 09:56:09.009777       1 shared_informer.go:280] Caches are synced for deployment
I1211 09:56:09.011373       1 shared_informer.go:280] Caches are synced for daemon sets
I1211 09:56:09.019733       1 shared_informer.go:280] Caches are synced for job
I1211 09:56:09.022328       1 shared_informer.go:280] Caches are synced for persistent volume
I1211 09:56:09.028382       1 shared_informer.go:280] Caches are synced for ClusterRoleAggregator
I1211 09:56:09.036669       1 shared_informer.go:280] Caches are synced for PV protection
I1211 09:56:09.036786       1 shared_informer.go:280] Caches are synced for HPA
I1211 09:56:09.041348       1 shared_informer.go:280] Caches are synced for TTL
I1211 09:56:09.051652       1 shared_informer.go:280] Caches are synced for ReplicaSet
I1211 09:56:09.052266       1 shared_informer.go:280] Caches are synced for cronjob
I1211 09:56:09.052313       1 shared_informer.go:280] Caches are synced for stateful set
I1211 09:56:09.058334       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-serving
I1211 09:56:09.059749       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-client
I1211 09:56:09.059828       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1211 09:56:09.061219       1 shared_informer.go:280] Caches are synced for disruption
I1211 09:56:09.061305       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-legacy-unknown
I1211 09:56:09.065833       1 shared_informer.go:280] Caches are synced for certificate-csrapproving
I1211 09:56:09.065899       1 shared_informer.go:280] Caches are synced for service account
I1211 09:56:09.071477       1 shared_informer.go:280] Caches are synced for PVC protection
I1211 09:56:09.071586       1 shared_informer.go:280] Caches are synced for crt configmap
I1211 09:56:09.076709       1 shared_informer.go:280] Caches are synced for ReplicationController
I1211 09:56:09.078165       1 shared_informer.go:280] Caches are synced for node
I1211 09:56:09.078364       1 range_allocator.go:167] Sending events to api server.
I1211 09:56:09.078501       1 range_allocator.go:171] Starting range CIDR allocator
I1211 09:56:09.078524       1 shared_informer.go:273] Waiting for caches to sync for cidrallocator
I1211 09:56:09.078554       1 shared_informer.go:280] Caches are synced for cidrallocator
I1211 09:56:09.079387       1 shared_informer.go:280] Caches are synced for TTL after finished
I1211 09:56:09.081847       1 shared_informer.go:280] Caches are synced for GC
I1211 09:56:09.084276       1 shared_informer.go:280] Caches are synced for taint
I1211 09:56:09.084440       1 node_lifecycle_controller.go:1438] Initializing eviction metric for zone: 
I1211 09:56:09.084509       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
W1211 09:56:09.084604       1 node_lifecycle_controller.go:1053] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1211 09:56:09.084707       1 node_lifecycle_controller.go:1254] Controller detected that zone  is now in state Normal.
I1211 09:56:09.084742       1 taint_manager.go:211] "Sending events to api server"
I1211 09:56:09.084798       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1211 09:56:09.087283       1 shared_informer.go:280] Caches are synced for attach detach
I1211 09:56:09.107299       1 shared_informer.go:280] Caches are synced for bootstrap_signer
I1211 09:56:09.114877       1 shared_informer.go:280] Caches are synced for endpoint_slice
I1211 09:56:09.189316       1 shared_informer.go:280] Caches are synced for endpoint_slice_mirroring
I1211 09:56:09.194357       1 shared_informer.go:280] Caches are synced for endpoint
I1211 09:56:09.254670       1 shared_informer.go:280] Caches are synced for resource quota
I1211 09:56:09.254695       1 shared_informer.go:280] Caches are synced for resource quota
I1211 09:56:09.603578       1 shared_informer.go:280] Caches are synced for garbage collector
I1211 09:56:09.603628       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1211 09:56:09.610445       1 shared_informer.go:280] Caches are synced for garbage collector

* 
* ==> kube-proxy [803a1eb6c8ae] <==
* I1212 12:11:53.936362       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1212 12:11:53.936442       1 server_others.go:109] "Detected node IP" address="192.168.49.2"
I1212 12:11:53.936480       1 server_others.go:535] "Using iptables proxy"
I1212 12:11:53.999813       1 server_others.go:176] "Using iptables Proxier"
I1212 12:11:53.999838       1 server_others.go:183] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1212 12:11:53.999846       1 server_others.go:184] "Creating dualStackProxier for iptables"
I1212 12:11:53.999860       1 server_others.go:465] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1212 12:11:54.010300       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1212 12:11:54.012318       1 server.go:655] "Version info" version="v1.26.3"
I1212 12:11:54.012334       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1212 12:11:54.014436       1 config.go:317] "Starting service config controller"
I1212 12:11:54.014508       1 config.go:226] "Starting endpoint slice config controller"
I1212 12:11:54.015433       1 config.go:444] "Starting node config controller"
I1212 12:11:54.015863       1 shared_informer.go:273] Waiting for caches to sync for service config
I1212 12:11:54.015867       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I1212 12:11:54.015937       1 shared_informer.go:273] Waiting for caches to sync for node config
I1212 12:11:54.116000       1 shared_informer.go:280] Caches are synced for endpoint slice config
I1212 12:11:54.115999       1 shared_informer.go:280] Caches are synced for service config
I1212 12:11:54.116062       1 shared_informer.go:280] Caches are synced for node config

* 
* ==> kube-proxy [c2621343f984] <==
* E1211 09:55:44.023248       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I1211 09:55:52.379384       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1211 09:55:52.379502       1 server_others.go:109] "Detected node IP" address="192.168.49.2"
I1211 09:55:52.379587       1 server_others.go:535] "Using iptables proxy"
I1211 09:55:52.473348       1 server_others.go:176] "Using iptables Proxier"
I1211 09:55:52.473429       1 server_others.go:183] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1211 09:55:52.473471       1 server_others.go:184] "Creating dualStackProxier for iptables"
I1211 09:55:52.473518       1 server_others.go:465] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1211 09:55:52.473640       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1211 09:55:52.474711       1 server.go:655] "Version info" version="v1.26.3"
I1211 09:55:52.474761       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1211 09:55:52.476263       1 config.go:226] "Starting endpoint slice config controller"
I1211 09:55:52.476303       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I1211 09:55:52.476415       1 config.go:317] "Starting service config controller"
I1211 09:55:52.476435       1 shared_informer.go:273] Waiting for caches to sync for service config
I1211 09:55:52.477600       1 config.go:444] "Starting node config controller"
I1211 09:55:52.477627       1 shared_informer.go:273] Waiting for caches to sync for node config
I1211 09:55:52.576702       1 shared_informer.go:280] Caches are synced for service config
I1211 09:55:52.576866       1 shared_informer.go:280] Caches are synced for endpoint slice config
I1211 09:55:52.578691       1 shared_informer.go:280] Caches are synced for node config

* 
* ==> kube-scheduler [2bb794086464] <==
* I1212 12:11:48.509994       1 serving.go:348] Generated self-signed cert in-memory
W1212 12:11:50.713390       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1212 12:11:50.713555       1 authentication.go:349] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1212 12:11:50.713634       1 authentication.go:350] Continuing without authentication configuration. This may treat all requests as anonymous.
W1212 12:11:50.713645       1 authentication.go:351] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1212 12:11:50.724629       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.3"
I1212 12:11:50.724642       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1212 12:11:50.726417       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1212 12:11:50.726533       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1212 12:11:50.726570       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1212 12:11:50.726948       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1212 12:11:50.827136       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E1212 12:16:50.850570       1 event_broadcaster.go:253] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-controller-6cc5ccb977-77njn.17a0146b708cceb8", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"a317f3e9-885a-428f-91c8-818f1605d5d1", ResourceVersion:"117638", Generation:0, CreationTimestamp:time.Date(2023, time.December, 12, 12, 11, 50, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"kube-scheduler", Operation:"Update", APIVersion:"events.k8s.io/v1", Time:time.Date(2023, time.December, 12, 12, 11, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000aaa120), Subresource:""}}}, EventTime:time.Date(2023, time.December, 12, 12, 11, 50, 828378000, time.Local), Series:(*v1.EventSeries)(0xc0005641a0), ReportingController:"default-scheduler", ReportingInstance:"default-scheduler-minikube", Action:"Scheduling", Reason:"FailedScheduling", Regarding:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-6cc5ccb977-77njn", UID:"eb1f9cb5-88d9-45ba-910f-72ad6a66b578", APIVersion:"v1", ResourceVersion:"109498", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..", Type:"Warning", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedLastTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedCount:0}': 'Event "ingress-nginx-controller-6cc5ccb977-77njn.17a0146b708cceb8" is invalid: series.count: Invalid value: "": should be at least 2' (will not retry!)
E1212 12:16:50.850663       1 event_broadcaster.go:253] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create-rjg9h.17a0146b708ee343", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"643d352d-a800-4df5-b2f5-c4239eaafb0b", ResourceVersion:"117639", Generation:0, CreationTimestamp:time.Date(2023, time.December, 12, 12, 11, 50, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"kube-scheduler", Operation:"Update", APIVersion:"events.k8s.io/v1", Time:time.Date(2023, time.December, 12, 12, 11, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00042fdd0), Subresource:""}}}, EventTime:time.Date(2023, time.December, 12, 12, 11, 50, 828514000, time.Local), Series:(*v1.EventSeries)(0xc000921100), ReportingController:"default-scheduler", ReportingInstance:"default-scheduler-minikube", Action:"Scheduling", Reason:"FailedScheduling", Regarding:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-admission-create-rjg9h", UID:"ff02bcdd-3e2c-4e0d-aede-40521fdebf88", APIVersion:"v1", ResourceVersion:"109507", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..", Type:"Warning", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedLastTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedCount:0}': 'Event "ingress-nginx-admission-create-rjg9h.17a0146b708ee343" is invalid: series.count: Invalid value: "": should be at least 2' (will not retry!)
E1212 12:16:50.850952       1 event_broadcaster.go:253] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch-5wnvd.17a0146b708abad5", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"41b9f942-3b83-49e0-b2ee-d8860d899d67", ResourceVersion:"117637", Generation:0, CreationTimestamp:time.Date(2023, time.December, 12, 12, 11, 50, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"kube-scheduler", Operation:"Update", APIVersion:"events.k8s.io/v1", Time:time.Date(2023, time.December, 12, 12, 11, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0008ae900), Subresource:""}}}, EventTime:time.Date(2023, time.December, 12, 12, 11, 50, 828235000, time.Local), Series:(*v1.EventSeries)(0xc00009f2a0), ReportingController:"default-scheduler", ReportingInstance:"default-scheduler-minikube", Action:"Scheduling", Reason:"FailedScheduling", Regarding:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-admission-patch-5wnvd", UID:"1db9ca40-53f5-4c7a-b637-42ed59284714", APIVersion:"v1", ResourceVersion:"109511", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..", Type:"Warning", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedLastTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedCount:0}': 'Event "ingress-nginx-admission-patch-5wnvd.17a0146b708abad5" is invalid: series.count: Invalid value: "": should be at least 2' (will not retry!)

* 
* ==> kube-scheduler [5857654fa981] <==
* I1211 09:55:48.400648       1 serving.go:348] Generated self-signed cert in-memory
W1211 09:55:52.197943       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1211 09:55:52.198309       1 authentication.go:349] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1211 09:55:52.198504       1 authentication.go:350] Continuing without authentication configuration. This may treat all requests as anonymous.
W1211 09:55:52.207020       1 authentication.go:351] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1211 09:55:52.270227       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.3"
I1211 09:55:52.270456       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1211 09:55:52.276229       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1211 09:55:52.276864       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1211 09:55:52.277745       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1211 09:55:52.277817       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1211 09:55:52.377229       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E1211 10:00:52.402388       1 event_broadcaster.go:253] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch-5wnvd.179fbe6b55d4799b", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"7e1922e4-6e41-402d-ac53-0cba3e5a1683", ResourceVersion:"115945", Generation:0, CreationTimestamp:time.Date(2023, time.December, 11, 9, 55, 52, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"kube-scheduler", Operation:"Update", APIVersion:"events.k8s.io/v1", Time:time.Date(2023, time.December, 11, 9, 55, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003e1c50), Subresource:""}}}, EventTime:time.Date(2023, time.December, 11, 9, 55, 52, 380076000, time.Local), Series:(*v1.EventSeries)(0xc0000a33a0), ReportingController:"default-scheduler", ReportingInstance:"default-scheduler-minikube", Action:"Scheduling", Reason:"FailedScheduling", Regarding:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-admission-patch-5wnvd", UID:"1db9ca40-53f5-4c7a-b637-42ed59284714", APIVersion:"v1", ResourceVersion:"109511", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..", Type:"Warning", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedLastTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedCount:0}': 'Event "ingress-nginx-admission-patch-5wnvd.179fbe6b55d4799b" is invalid: series.count: Invalid value: "": should be at least 2' (will not retry!)
E1211 10:01:22.394326       1 event_broadcaster.go:253] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-controller-6cc5ccb977-77njn.179fbe6b5653cec0", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"59c145a0-6692-4c61-9fce-866cccb8d7d4", ResourceVersion:"115944", Generation:0, CreationTimestamp:time.Date(2023, time.December, 11, 9, 55, 52, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"kube-scheduler", Operation:"Update", APIVersion:"events.k8s.io/v1", Time:time.Date(2023, time.December, 11, 9, 55, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0001962e8), Subresource:""}}}, EventTime:time.Date(2023, time.December, 11, 9, 55, 52, 388438000, time.Local), Series:(*v1.EventSeries)(0xc00070a140), ReportingController:"default-scheduler", ReportingInstance:"default-scheduler-minikube", Action:"Scheduling", Reason:"FailedScheduling", Regarding:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-6cc5ccb977-77njn", UID:"eb1f9cb5-88d9-45ba-910f-72ad6a66b578", APIVersion:"v1", ResourceVersion:"109498", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..", Type:"Warning", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedLastTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedCount:0}': 'Event "ingress-nginx-controller-6cc5ccb977-77njn.179fbe6b5653cec0" is invalid: series.count: Invalid value: "": should be at least 2' (will not retry!)
E1211 10:01:22.394330       1 event_broadcaster.go:253] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create-rjg9h.179fbe6b5634bce0", GenerateName:"", Namespace:"ingress-nginx", SelfLink:"", UID:"139c01d2-b431-49f3-beee-980eecaccbfe", ResourceVersion:"115943", Generation:0, CreationTimestamp:time.Date(2023, time.December, 11, 9, 55, 52, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"kube-scheduler", Operation:"Update", APIVersion:"events.k8s.io/v1", Time:time.Date(2023, time.December, 11, 9, 55, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003e1c98), Subresource:""}}}, EventTime:time.Date(2023, time.December, 11, 9, 55, 52, 386396000, time.Local), Series:(*v1.EventSeries)(0xc00089e180), ReportingController:"default-scheduler", ReportingInstance:"default-scheduler-minikube", Action:"Scheduling", Reason:"FailedScheduling", Regarding:v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-admission-create-rjg9h", UID:"ff02bcdd-3e2c-4e0d-aede-40521fdebf88", APIVersion:"v1", ResourceVersion:"109507", FieldPath:""}, Related:(*v1.ObjectReference)(nil), Note:"0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..", Type:"Warning", DeprecatedSource:v1.EventSource{Component:"", Host:""}, DeprecatedFirstTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedLastTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeprecatedCount:0}': 'Event "ingress-nginx-admission-create-rjg9h.179fbe6b5634bce0" is invalid: series.count: Invalid value: "": should be at least 2' (will not retry!)
I1211 10:27:31.710588       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1211 10:27:31.712442       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I1211 10:27:31.712554       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1211 10:27:31.712781       1 scheduling_queue.go:1072] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E1211 10:27:31.712848       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* -- Logs begin at Tue 2023-12-12 12:11:23 UTC, end at Tue 2023-12-12 12:30:37 UTC. --
Dec 12 12:24:21 minikube kubelet[1641]: E1212 12:24:21.118770    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:24:33 minikube kubelet[1641]: I1212 12:24:33.115782    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:24:33 minikube kubelet[1641]: E1212 12:24:33.118823    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:24:44 minikube kubelet[1641]: I1212 12:24:44.116522    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:24:44 minikube kubelet[1641]: E1212 12:24:44.120643    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:24:56 minikube kubelet[1641]: I1212 12:24:56.117641    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:24:56 minikube kubelet[1641]: E1212 12:24:56.128668    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:25:11 minikube kubelet[1641]: I1212 12:25:11.115759    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:25:11 minikube kubelet[1641]: E1212 12:25:11.126325    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:25:22 minikube kubelet[1641]: I1212 12:25:22.115923    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:25:22 minikube kubelet[1641]: E1212 12:25:22.119595    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:25:33 minikube kubelet[1641]: I1212 12:25:33.114869    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:25:33 minikube kubelet[1641]: E1212 12:25:33.117952    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:25:45 minikube kubelet[1641]: I1212 12:25:45.115540    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:25:45 minikube kubelet[1641]: E1212 12:25:45.119146    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:25:56 minikube kubelet[1641]: I1212 12:25:56.116793    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:25:56 minikube kubelet[1641]: E1212 12:25:56.120533    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:26:08 minikube kubelet[1641]: I1212 12:26:08.116152    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:26:08 minikube kubelet[1641]: E1212 12:26:08.120546    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:26:22 minikube kubelet[1641]: I1212 12:26:22.115843    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:26:22 minikube kubelet[1641]: E1212 12:26:22.119146    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:26:34 minikube kubelet[1641]: I1212 12:26:34.115025    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:26:34 minikube kubelet[1641]: E1212 12:26:34.116688    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:26:46 minikube kubelet[1641]: I1212 12:26:46.116673    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:26:46 minikube kubelet[1641]: E1212 12:26:46.120990    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:26:58 minikube kubelet[1641]: I1212 12:26:58.115487    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:26:58 minikube kubelet[1641]: E1212 12:26:58.119647    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:27:12 minikube kubelet[1641]: I1212 12:27:12.115531    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:27:12 minikube kubelet[1641]: E1212 12:27:12.118840    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:27:27 minikube kubelet[1641]: I1212 12:27:27.115144    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:27:27 minikube kubelet[1641]: E1212 12:27:27.118469    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:27:38 minikube kubelet[1641]: I1212 12:27:38.115545    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:27:38 minikube kubelet[1641]: E1212 12:27:38.118808    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:27:50 minikube kubelet[1641]: I1212 12:27:50.115068    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:27:50 minikube kubelet[1641]: E1212 12:27:50.118251    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:28:05 minikube kubelet[1641]: I1212 12:28:05.115513    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:28:05 minikube kubelet[1641]: E1212 12:28:05.118856    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:28:16 minikube kubelet[1641]: I1212 12:28:16.116496    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:28:16 minikube kubelet[1641]: E1212 12:28:16.120474    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:28:29 minikube kubelet[1641]: I1212 12:28:29.115969    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:28:38 minikube kubelet[1641]: E1212 12:28:38.893634    1641 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for django-app-v1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="docker.io/library/django-app-v1:latest"
Dec 12 12:28:38 minikube kubelet[1641]: E1212 12:28:38.893735    1641 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for django-app-v1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="docker.io/library/django-app-v1:latest"
Dec 12 12:28:38 minikube kubelet[1641]: E1212 12:28:38.893925    1641 kuberuntime_manager.go:872] container &Container{Name:django-container,Image:docker.io/library/django-app-v1:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z88jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod django-deployment-6c4bb6586c-jw5bv_django-app-ns(dfb2232f-5573-4868-8dfa-6ace0d8904a4): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for django-app-v1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 12 12:28:38 minikube kubelet[1641]: E1212 12:28:38.894024    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for django-app-v1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:28:54 minikube kubelet[1641]: I1212 12:28:54.115512    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:28:54 minikube kubelet[1641]: E1212 12:28:54.120124    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:29:07 minikube kubelet[1641]: I1212 12:29:07.115175    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:29:07 minikube kubelet[1641]: E1212 12:29:07.117201    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:29:21 minikube kubelet[1641]: I1212 12:29:21.114803    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:29:21 minikube kubelet[1641]: E1212 12:29:21.116003    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:29:34 minikube kubelet[1641]: I1212 12:29:34.115832    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:29:34 minikube kubelet[1641]: E1212 12:29:34.119296    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:29:49 minikube kubelet[1641]: I1212 12:29:49.115760    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:29:49 minikube kubelet[1641]: E1212 12:29:49.118985    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:30:00 minikube kubelet[1641]: I1212 12:30:00.114992    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:30:00 minikube kubelet[1641]: E1212 12:30:00.118487    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:30:13 minikube kubelet[1641]: I1212 12:30:13.115017    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:30:13 minikube kubelet[1641]: E1212 12:30:13.118198    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4
Dec 12 12:30:25 minikube kubelet[1641]: I1212 12:30:25.115609    1641 scope.go:115] "RemoveContainer" containerID="9620957d156cb4d61f752a5c0ea0a9f2b5b5aef31a833d6af4ae2d4f4c26a17a"
Dec 12 12:30:25 minikube kubelet[1641]: E1212 12:30:25.118730    1641 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"django-container\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/library/django-app-v1:latest\\\"\"" pod="django-app-ns/django-deployment-6c4bb6586c-jw5bv" podUID=dfb2232f-5573-4868-8dfa-6ace0d8904a4

* 
* ==> kubernetes-dashboard [a7511e58b96a] <==
* 2023/12/11 09:55:45 Using namespace: kubernetes-dashboard
2023/12/11 09:55:45 Using in-cluster config to connect to apiserver
2023/12/11 09:55:45 Using secret token for csrf signing
2023/12/11 09:55:45 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/12/11 09:55:52 Successful initial request to the apiserver, version: v1.26.3
2023/12/11 09:55:52 Generating JWE encryption key
2023/12/11 09:55:52 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/12/11 09:55:52 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/12/11 09:55:53 Initializing JWE encryption key from synchronized object
2023/12/11 09:55:53 Creating in-cluster Sidecar client
2023/12/11 09:55:53 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/11 09:55:53 Serving insecurely on HTTP port: 9090
2023/12/11 09:56:23 Successful request to sidecar
2023/12/11 09:55:45 Starting overwatch

* 
* ==> kubernetes-dashboard [b8d4aecc5279] <==
* 2023/12/12 12:11:54 Using namespace: kubernetes-dashboard
2023/12/12 12:11:54 Using in-cluster config to connect to apiserver
2023/12/12 12:11:54 Using secret token for csrf signing
2023/12/12 12:11:54 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/12/12 12:11:54 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/12/12 12:11:54 Successful initial request to the apiserver, version: v1.26.3
2023/12/12 12:11:54 Generating JWE encryption key
2023/12/12 12:11:54 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/12/12 12:11:54 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/12/12 12:11:55 Initializing JWE encryption key from synchronized object
2023/12/12 12:11:55 Creating in-cluster Sidecar client
2023/12/12 12:11:55 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 12:11:55 Serving insecurely on HTTP port: 9090
2023/12/12 12:12:25 Successful request to sidecar
2023/12/12 12:11:54 Starting overwatch

* 
* ==> storage-provisioner [b9ef81c4b8f9] <==
* I1212 12:12:39.361909       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1212 12:12:39.377959       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1212 12:12:39.378566       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1212 12:12:56.804170       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1212 12:12:56.804347       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"181479b7-4acb-47e6-bdde-b3a53082d3a8", APIVersion:"v1", ResourceVersion:"117821", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_269ce448-1f2c-495e-a019-ebbc6f5abd7b became leader
I1212 12:12:56.804481       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_269ce448-1f2c-495e-a019-ebbc6f5abd7b!
I1212 12:12:56.906312       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_269ce448-1f2c-495e-a019-ebbc6f5abd7b!

* 
* ==> storage-provisioner [bc16084884c4] <==
* I1212 12:11:53.351663       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1212 12:12:23.481461       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

